{
  "timestamp": "2025-07-02_10-51-55",
  "active_window": "ChatGPT",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "curl -X POST http://127.0.0.1:8000/generate \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"scenario\":\"missed class\",\"urgency\":\"panic\"}'",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv(), override=True)\n\n\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nimport google.generativeai as genai\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\"))\nmodel = genai.GenerativeModel(\"models/gemini-pro\")\n#print(\">>> model being used:\", model._name) \n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    full_prompt = f\"{SYSTEM_PROMPT}\\nScenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    out = json.loads(model.generate_content([full_prompt]).text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]",
  "ocr_text": "@ ChatGPT File Edit View Window\u2019 Help 6 \u20ac8 \u00a9 DW S 32%%) F Q S \u00a9 Wed Jul2 10:51AM\n= List of research @ Artificial Intellige CO NLP_1.ipynb - Co CO NLP_1.ipynb - Co ca LAUNCHED Glob M Launched - Artif Artificial_Intelligence | Al-05-BBLEN4 \u00a9 Openal o API keys - Open/ (G) Gemini API refers * Get API key | G X\n\nOw \u00ae OF\n\n\u20ac > C\n\nGoogle Al Studio\n\nO & \u00a9 aistudio.google.com/apikey\n\nEXPLORER\noa\n\nV EXCUSE-GENERATOR\nP > __pycache__\nil, Us:\n\nv venv\n\u00bb > bin\n= Ch > include\nS > lib\n& \u00a9 pyvenv.cfg U\n .env\n\nBS @ excuse_api.py 4,U\n\n{} history.json U\nA = requirements.txt U\n\n@\n$03 > OUTLINE\n\n> TIMELINE\n. in* @& 0A4\nView stat All men es ees\n\n\u20ac_ 5 PP excuse-generator By 08 DO ft\n\nx Welcome @ excuse_api.py 4,U @\u00ae % .env U Dy %\n\n@ excuse_api.py > ...\n\n40 def generate(r: Req):\n\n41 prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n42 full_prompt = f\"{SYSTEM_PROMPT}\\nScenario: {r.scenario}\\nUrgency:\n43 out = json. loads(model.generate_content([full_prompt] ).text)\n44 entry = {\n45 \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\nPROBLEMS @ OUTPUT DEBUG CONSOLE TERMINAL PORTS +yv cee\n7, in run -| 23) pyth\u00a2\nresult = context.run(func, *args) 2) zsh\n\nFile \"/Users/bhuvanarora/excuse-generator/excuse_api.py\", line 43, in generate\nout = json. loads (model. generate_content(full_prompt) .text)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/generativeai/generative_models\n-py\", line 331, in generate_content\nresponse = self._client.generate_content(\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/ai/generat ive language_vibeta/s\nervices/generative_service/client.py\", line 835, in generate_content\nresponse = rpc(\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method. py\",\nline 131, in __call__\nreturn wrapped_func(xargs, **kwargs) o\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\"\n, line 294, in retry_wrapped_func\nreturn retry_target(\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\"\n, line 156, in retry_target\nnext_sleep = _retry_error_helper(\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\",\nline 214, in _retry_error_helper\nraise final_exc from source_exc\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\"\n, line 147, in retry_target\nresult = target()\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130\n, in func_with_timeout\nreturn func(*args, +kkwargs)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", lin\ne 78, in error_remapped_callable\nraise exceptions. from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.NotFound: 404 models/gemini-pro is not found for API version vibeta, or is not supported\nfor generateContent. Call ListModels to see the list of available models and their supported methods.\n\nLn62,Col23 Spaces:4 UTF-8 LF {} Python @& 3.13.2 64-\n\nnb mH se &  \u201c &\nSSOvE\n\n.\n\neco MO @G\n\nBhuvan\n\nThought for 6 seconds\n\n@ 1 file changed\n\nB excuse_api.py +2 \u20142\n\n12\n13\n14\n15\n16\n\n\u00bb\u201c\n\n41\n7)\n\n43\n44\n45\n\nIntelligent Excuse Generator Plan >\n\nimport google.generativeai as genai\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\") )\n\nmodel\n\nmodel\n\n24 lines\n\ngenai.GenerativeModel(\"gemini-pro\" )\ngenai.GenerativeModel(\"models/gemini-pro\" )\n\nReview\n\nprompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\nfull_prompt = f\"{SYSTEM_PROMPT}\\nScenario: {r.scenario}\\nUrgency:\n\n{r.urgency}\"\nout\n\nout\nentry = {\n\n\"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n\nI've updated excuse_api.py so it now:\n\n+ Builds the model with the full resource name models/gemini-pro\n\njson.loads(model.generate_content(full_prompt) .text)\njson. loads(model.generate_content([full_prompt]).text)\n\n+ Calls generate_content([full_prompt]) (the list form the API expects)\n\nRestart Uvicorn:\n\nbash\n\nCtr1-C\n\nsource .venv/bin/activate\n\nuvicorn excuse_api:app --reload v\n\n\u00a9 excuse_api.py x\n\nMessage Bhuvan\n\n+ @ &\n\naSee2o \u00a9oe20\n\n.\n\nnit\n\n.\n\n@ Revert\n\nfa)\n\n\u00a9) Copy"
}