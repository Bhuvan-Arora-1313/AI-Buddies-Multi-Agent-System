{
  "timestamp": "2025-07-02_10-33-43",
  "active_window": "ChatGPT",
  "focused_text": "p",
  "clipboard": "curl -X POST http://127.0.0.1:8000/generate \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"scenario\":\"missed class\", \"urgency\":\"panic\"}'",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv(), override=True)\n\n\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nimport google.generativeai as genai\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\"))\nmodel = genai.GenerativeModel(\"gemini-pro\")\n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    full_prompt = f\"{SYSTEM_PROMPT}\\n{prompt}\"\n    out = json.loads(model.generate_content(full_prompt).text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]",
  "ocr_text": "6\n\no\nS\n\n\u00a9\n\nChatGPT File Edit View\n\n= List of research\n\nC2 GA\n\nEXPLORER\n\n| \\Y EXCUSE-GENERATOR\n\nP > __pycache__\nVv wenv\n\n> bin\nae\n\n> include\n\n> lib\n\n\u00a9 pyvenv.cfg\n\u00a9 .env\n\nES @ excuse_api.py 4,\n\n{} history.json\n\nU\nU\nU\nU\nA = requirements.txt U\n\n@\n> OUTLINE\na > TIMELINE\n\nGE & maine @0A4\n\nWindow\n\nHelp\n\n* Artificial Intellige CO NLP_1.ipynb - Co\n\nO 8 =\n\naistudio.google.com/apikey\n\nCO NLP_1.ipynb - Cc\n\nBB LAUNCHED Globz = M Launched - Artific _Artificial_Intelligence\n\n< P excuse-generator By BOW a\nPROBLEMS @ OUTPUT DEBUGCONSOLE TERMINAL PORTS tye vy x\ngrep: #: No such file or directory | >.) python3.11\ngrep: should: No such file or directory >] zsh\ngrep: show: No such file or directory ' ~\ngrep: @.3.x: No such file or directory\ngrep: or: No such file or directory\ngrep: newer: No such file or directory\nERROR: Pipe to stdout was broken\nException ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\nBrokenPipeError: [Errno 32] Broken pipe F\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % pip show google-generativeai | grep Version\nVersion: 0.8.5\n(.venv) bhuvanarora@Bhuvans-MacBook-Pro excuse-generator % uvicorn excuse_api:app --reload\nINFO: Will watch for changes in these directories: ['/Users/bhuvanarora/excuse-generator']\nINFO Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nINFO Started reloader process [97505] using StatReload\nINFO Started server process [97507] cl\nINFO Waiting for application startup. I\nINFO Application startup complete.\nINFO: 127.@.0.1:65131 - \"POST /generate HTTP/1.1\" 520 Internal Server Error\nERROR: Exception in ASGI application\n\nTraceback (most recent call last):\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.\n\nline 403, in run_asgi\nresult = await app( # type: ignore[func-returns-value]\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1ib/python3\nline 60, in __call__\nreturn await self.app(scope, receive, send)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3\nin __call__\nawait super().__call__(scope, receive, send)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3\nin __call.\nawait self.middleware_stack(scope, receive, send)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3\n187, in __call__\nraise exc\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3\n165, in __call__\nawait self.app(scope, receive, _send)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3\nline 62, in __call__\nawait wrap_app_handling_exceptions(self.app, conn) (scope,\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.\n\ne 53, in wrapped_app\nraise exc\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3\ne 42, in wrapped_app\nawait app(scope, receive, sender)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3\ncall__\nawait self.middleware_stack(scope, receive, send)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3\npp\nawait route.handle(scope, receive, send)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3\nandle\nawait self.app(scope, receive, send)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3\n\nfile.pdf\n\nITRV (1).pdf ITRV.pdf\n\n. . .\n\n-11/site-packages/fastapi/applications.py\", line 1054,\n\n11/site-packages/uvicorn/protocols/http/h11_impl. py\",\n\n.11/site-packages/uvicorn/middleware/proxy_headers.py\",\n\n-11/site-packages/starlette/applications.py\", line 112,\n\n-11/site-packages/starlette/middleware/errors.py\", line\n\n-11/site-packages/starlette/middleware/errors.py\", line\n\n-11/site-packages/starlette/middleware/exceptions.py\", l\n\nreceive, send)\n11/site-packages/starlette/_exception_handler.py\", lin\n\n-11/site-packages/starlette/_exception_handler.py\", lin\nt\n\n-11/site-packages/starlette/routing.py\", line 714, in _\n\nt\n-11/site-packages/starlette/routing.py\", line 734, ina\n-11/site-packages/starlette/routing.py\", line 288, in h\n\n-11/site-packages/starlette/routing.py\", line 76, in ap\n\n{} Python & 3.13.264-bit\n\nIko to MTJ (1).pdf\n\nse20\n\nQ Ln7,Col1 Spaces:4 UTF-8 LF\n\nJETIR2101184\n\nIko to mtj\n\n(B\u00ae al-05-BBLeEN4\n\n6 fF OCOO8\n@ apikeys Open\n\n2a%*t) FQ SB\n\n\u00a9 Opendl \u00a9 Gemini APirefere | %@ Get API key | G X\n\neco MO @G\n\nIko to MTJ-s-=\n\nIntelligent Excuse Generator Plan >\nBhuvan\n\nthe terminal so pip can run without conflicts.\n\nOvad gy\n\nThought for 10 seconds >\nThought for a couple of seconds\n\nHow did the test go after switching to models/gemini-pro and restarting Uvicorn\u2014did the\ncur] call finally return a JSON excuse, or are you still seeing an error message?\n\nLet me know the outcome (or paste the last few traceback lines) and we'll get you fully up and\nrunning.\n\nOvad Yd\n\n\u20ac Work with Code Tab x\n\npl\n+ \u00ae@ &\n\n@ Wed Jul 2 10:33AM"
}