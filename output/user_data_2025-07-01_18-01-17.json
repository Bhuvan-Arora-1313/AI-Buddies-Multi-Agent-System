{
  "timestamp": "2025-07-01_18-01-17",
  "active_window": "firefox",
  "focused_text": "\u200b\u200b",
  "clipboard": "git push",
  "vscode_text": "<!DOCTYPE html>\n<html>\n  <body style=\"margin: 0; background: rgba(0,0,0,0.7); color: white; font-family: sans-serif;\">\n    <div style=\"-webkit-app-region: drag; padding: 10px; background: rgba(0,0,0,0.8);\">\n      <pre id=\"output\" style=\"white-space: pre-wrap; word-wrap: break-word;\">Waiting for data...</pre>\n    </div>\n    <script>\n      const { ipcRenderer } = require('electron');\n      ipcRenderer.on('update-prediction', (event, data) => {\n        document.getElementById('output').textContent = data;\n      });\n    </script>\n  </body>\n</html>",
  "ocr_text": "nay\n\n= List of research papers - Google X\n\n\u20ac C a\n\nO68\nY ChatGPT\n\n=O ChatGPT v\n\nSaved memory full \u00a9\n\nfly together\u2014it depends on the mission.\n\nDo you want me to explain how drones stay\n\nconnected when they are dispersed?\n\nAOgPmerv &\n\nppo algo\n\nThe PPO algorithm stands for Proximal\nPolicy Optimization.\nIt is a popular Reinforcement Learning (RL)\n\nalgorithm.\n\nLet me explain in simple words:\n\n@ What is PPO?\n\ne PPO is a method vy to train agents\n\n(like drones, robots, or game\n\nAsk anything\n\n+ 2 Tools Q\n\nChatGPT can make mistakes. Check important info. See Cookie\nPreferences.\n\nN_ Enhanced multi agent coordinat X\n\n* s41598-025-88145-7-2. pdf x\n\ngoogle.com\n\n\u00a9) JayKap-Git/Buday\n\nList of research papers + B\nTools Extensions Help\n\nFile Edit View Insert\n\nQo @e@ BA F 100%\n\n<\n\nDocument tabs +\n\nNovelty\n\nTraffic Patrolling Rout...\nSummary\nHighlight\n\nNovelty\n\nSummary\nHighlight\nNovelty\nA UAV Patrol System ...\nSummary\nHighlight\nNovelty\nSmart Border Patrol U...\nSummary\nHighlight\nNovelty\nUAV-Enabled Intellige...\nSummary\nHighlight\n\nNovelty\n\nFormat\n\nv\n\nNormal text\n\n1\nro\n\nx of\nw\n\n)\n\n~ Avl oo\u00bb |-[(nj+ BrU AS \u00a9 f @ | S- te ve yey SEY\nmeee nS i oe i ee i i i i i Sn ns\n\nfor take-off/landing times, drone endurance, and uses mixed-integer programming for path\nplanning.\n\nNovelty\n\nUnlike earlier works on vehicle-drone coordination (which often focus on logistics), this paper\nextends the problem to urban traffic patrolling with road-based constraints. The DL-ARP\nformulation, inclusion of heterogeneous task types, and real-world deployment-backed\ncase study set it apart as a foundational work for future urban surveillance patrol systems.\n\n1.8. Enhanced Multi-Agent Coordination Algorithm for Drone Swarm\nPatrolling in Durian Orchards\nMarch 2025\nJournal : Scientific Reports\n\nPublisher : Nature Publishing Group\n\nSummary\n\nThis study introduces a novel EN-MASCA (Enhanced Multi-Agent Swarm Control Algorithm)\nfor optimizing drone swarm patrolling in complex agricultural environments\u2014specifically durian\norchards. The approach incorporates a virtual navigator model that dynamically guides drones\nin real-time, responding to environmental challenges such as obstacles, terrain irregularities,\nand wind.\n\nThe system uniquely combines Deep Q-Network (DQN) and Proximal Policy Optimization\n(PPO) reinforcement learning algorithms to refine flight path, speed, and obstacle avoidance.\nExtensive simulations using ROS-Gazebo and Rflysim3D environments validate its\neffectiveness compared to existing control algorithms (MASCA, NNCA, NSGA-II).\n\nHighlight\n\na"
}