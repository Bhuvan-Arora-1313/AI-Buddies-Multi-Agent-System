{
  "timestamp": "2025-07-01_17-48-48",
  "active_window": "firefox",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "git push",
  "vscode_text": "<!DOCTYPE html>\n<html>\n  <body style=\"margin: 0; background: rgba(0,0,0,0.7); color: white; font-family: sans-serif;\">\n    <div style=\"-webkit-app-region: drag; padding: 10px; background: rgba(0,0,0,0.8);\">\n      <pre id=\"output\" style=\"white-space: pre-wrap; word-wrap: break-word;\">Waiting for data...</pre>\n    </div>\n    <script>\n      const { ipcRenderer } = require('electron');\n      ipcRenderer.on('update-prediction', (event, data) => {\n        document.getElementById('output').textContent = data;\n      });\n    </script>\n  </body>\n</html>",
  "ocr_text": "N_ Enhanced multi agent coordinat X\n\n=} List of research papers - Google X\n\nC a\n\nY ChatGPT \u201cxX en)\n\n=O ChatGPT v G\n\nSaved memory full \u00a9\n\n\u00ae When Drones Stay Closer\n(Like a Flock):\ne Purpose: Cooperate tightly for a goal.\ne Examples:\ne Light shows\nCargo transport in groups\n\nDefense swarms attacking together\nIn these cases, moving together like\n\nbirds is useful.\n\n\u00bb Key Point:\n\nSwarm behavior + Always flying close.\nIt means they coordinate and cooperate.\nSometimes they spread out, sometimes they\n\nfly together\u2014it depends on the mission.\n\nDo you want me to explain how drones stay\n\nconnected when the \\V \u2018e dispersed?\ni a rs RK\n\nAsk anything\n\n+ 2 Tools Q\n\nChatGPT can make mistakes. Check important info. See Cookie\nPreferences.\n\naw\n\nVv\n\n* s41598-025-88145-7-2.pdf\n\n9 of 17\n\nx\n\n\u00a9) JayKap-Git/Buday\n\nOD ffile:///Users/bhuvanarora/Downloads/s41598-025-88145-7-2.pdf\n\nw\n\n\u2014 + 160% v\n\nFormula > Gleader YeWaras the leader lor ge g closer to the de\nthe cluster center for shrinking the distance from the leader. The distance betwd@eMleneaaty\nis rewards the cluster center for avoiding obstacles. Its network input is the na C) ee @\ncluster\u2019s center position. The distance vector between it and the obstacle, and the\nof the navigator. In order to achieve cooperative behavior of drone clusters, thigiiineares\nagent control strategy. Distributed control is achieved between drones through lod e\nexcessive reliance on centralized control. It also dynamically adjusts parameter\ndistance and speed difference between agents to ensure that the cluster maintai\nduring patrols. Finally, based on the group behavior model of Olfati-Saber, it intygavgll\nmethod to dynamically adjust the flight path to avoid collisions between drones a\nthe DQN and PPO algorithms, this study uses the target network and experience replay mechanism to reduce\nthe instability of model training, and uses parallel computing to accelerate the execution of path planning and\nobstacle avoidance strategies. It also uses the Bayesian optimization method to dynamically adjust parameters\nsuch as learning rate and discount factor to improve the robustness of the algorithm in complex environments.\n\n(eee\n\n| @)\n\nCOUDSTACIeS. DY COMID\n\nEnhanced multi-agent cluster model construction\n\nBefore building the model, this study set the learning rates of the DQN and PPO algorithms to 0.001 and\n0.0003. The learning rate of DQN is mainly based on the convergence requirements of the algorithm in\na dynamic environment, and a higher learning rate is selected to accelerate initial learning. The learning\nrate of PPO is based on empirical values and experimental tuning results, and a lower learning rate is\nselected to avoid oscillation or instability caused by too fast gradient updates in complex scenarios.\nThis study also set the batch size of DQN and PPO to 64 and 128. The smaller batch size of DQN is\nintended to improve the real-time update capability of the model, while the larger batch size of PPO helps\nto enhance the adaptability to complex environments and training stability. The discount factor is used\nto weigh short-term rewards and long-term benefits. This study set it to 0.99, highlighting the overall\nperformance of the drone swarm in long-term patrol missions and retaining the effectiveness of short-\nterm path optimization. The target network of the DQN algorithm is updated every 100 iterations. Based\non the consideration of model training stability, it delays the update of the target network to avoid training\noscillations caused by frequent updates. The PPO algorithm uses gradient clipping technology and sets the\ngradient clipping threshold to 0.2 to ensure that the gradient update does not exceed a reasonable range,\nthereby improving the stability of the model.\n\nIn order to verify the effectiveness and correctness of the algorithm, this study also built a multi- drone motion\nsimulation model and uses the four-degree-of-freedom 8-state drone dynamics model*\u2019. It contains 8 state\nvariables [s1, s2, s3, s4, s5, s6, s7, s8] and four input variables: the aileron deflection command Qa, elevator\nand rudder deflection command 9\u00bb, rudder angle command 9. and throttle command Q4q. In order to control\nthe drone swarms, this study uses three cluster control quantities: speed Vi, yaw angle R; and altitude H;. The\ndynamic model of the drone with attitude control capability is controlled through three cluster control quantities.\nThe output command of the cluster algorithm is converted into the input of the attitude control loop in the drone\ndynamic model. The PI control loop needs to be built as the transition. The relationship between the cluster\nalgorithm, PI control loop, attitude control loop and state dynamics model is shown in Fig. 6. In the process\nof model implementation, it first constructs a four-degree-of-freedom drone dynamics model. By defining\nthe drone\u2019s 8 state variables and 4 input variables, the drone dynamics equation is established to describe the\nrelationship between the state variables and the input variables. Then it designs the PI control loop by setting the\nproportional gain (K,) and integral gain parameters (K;) of the PI controller, which inputs the output command"
}