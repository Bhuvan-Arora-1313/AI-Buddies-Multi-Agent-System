{
  "timestamp": "2025-07-02_10-21-43",
  "active_window": "ChatGPT",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "curl -X POST http://127.0.0.1:8000/generate \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"scenario\":\"missed class\", \"urgency\":\"panic\"}'",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nload_dotenv()          # \u2190 NEW: pulls OPENAI_API_KEY from .env\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nimport google.generativeai as genai\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\"))\nmodel = genai.GenerativeModel(\"gemini-pro\")\n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    full_prompt = f\"{SYSTEM_PROMPT}\\n{prompt}\"\n    out = json.loads(model.generate_content(full_prompt).text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]",
  "ocr_text": "\u00ab\n\no\nS\n\n\u00a9\n\nChatGPT File Edit View\n= List of research\nSerena\nEXPLORER\n\n2\n\nP > __pycache__\nv .venv\n> bin\noe > include\n> lib\n\n<\n\n\u00a9 pyvenv.cfg\n\u00a9 .env\n\nre\nA\n\n@ excuse_api.py\n\n{} history.json\n= requirements.txt\n\n@\n> OUTLINE\na > TIMELINE\n\nGE & maine @0A4\n\n\\Y EXCUSE-GENERATOR\n\n4\n\nWindow\n\nU\nU\nU\nU\nU\n\nHelp\n\n* Artificial Intellige CO NLP_1.ipynb - Co CO NLP_1.ipynb - Cc ca LAUNCHED Glok \u2122 Launched - Artif\n\nO 8 =\n\naistudio.google.com/apikey\n\n<5 PP excuse-generator By\nJ Welcome \u00ae excuse_api.py 4,U X %.env U\n\u00ae excuse_api.py > \u00a9 generate\n33\n34 # -\u2014--------\u2014- /generate \u2014-------\u2014-\n35 @app.post(\"/generate\")\n36 def generate(r: Req):\n37 prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n38 full_prompt = f\"{SYSTEM_PROMPT}\\n{prompt}\"\n39 out = json. loads(model.generate_content(full_prompt) .text)\nprostems @ ourur DeBus consoue TERMINAL PORTS\n\nFile \"/Users/bhuvanarora/excuse-generator/excuse_api.py\",\nout =\n\nline 39, in generate\njson. loads (model. generate_content(full_prompt) . text)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/generativeai/generative_models\n-py\", line 331, in generate_content\nresponse = self._client.generate_content(\nFile \"/Users/bhuvanarora/excuse-generator/. venv/1ib/python3. 11/site-packages/google/ai/generativelanguage_' vibeta/s .\nervices/generative_service/client.py\", line 835, in generate_content\nresponse = rpc(\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method. py\",\nline 131, in __call__\nreturn wrapped_func(xargs, **kwargs)\nFile \"/Users/bhuvanarora/excuse-generator/.\n, line 294, in retry_wrapped_func\nreturn retry_target(\nFile \"/Users/bhuvanarora/excuse-generator/.\n, line 156, in retry_target\nnext_sleep = _retry_error_helper(\n\nvenv/1ib/python3.11/site-packages/google/api_core/retry/retry_unary.py\"\n\nvenv/1ib/python3\n\nFile \"/Users/bhuvanarora/excuse-generator/.\nline 214, in _retry_error_helper\nraise final_exc from source_exc\nFile \"/Users/bhuvanarora/excuse-generator/.\n, line 147, in retry_target\nresult = target()\n\nvenv/1ib/python3.11/site-packages/google/api_core/retry/retry_base.py\",\n\nvenv/1ib/python3.11/site-packages/google/api_core/retry/retry_unary.py\"\n\nFile \"/Users/bhuvanarora/excuse-generator/.\n, in func_with_timeout\nreturn func(*args, +kkwargs)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3\ne 78, in error_remapped_callable\nraise exceptions. from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.NotFound: 404 models/gemini-pro is not found for API version vibeta, or is not supported\nfor generateContent. Call ListModels to see the list of available models and their supported methods\n\nvenv/1ib/python3.11/site-packages/google/api_core/timeout.py\", line 130\n\n.11/site-packages/google/api_core/grpc_helpers.py\", lin\n\nLn 39, Col 63 Spaces 4 UTF-8 LF {} Python\n\nfile.pdf\n\nITRV (1).pdf ITRV.pdf AIEUTIR EAI EY\n\nIko to mtj\n\nArtificial_Intelligence\n\nBOW\n\n-11/site-packages/google/api_core/retry/retry_unary.py\"\n\nby\n\n0\n\nA x\n\npython3.11\nzsh\n\n&8 3.13.2 64-bit\n\nIko to MTJ (1).pdf = Iko to MTJ-s.\n\nOo\nLe}\n\nG@ROTO8\n\no API keys - Open\n\n2%8 ) >\n\nQ\n\nGB al-05-BBLEN4 \u00a9 Openal @ Gemini Api refer 96 Get API key | GX\n\nIntelligent Excuse Generator Plan >\nBhuvan\n\neco MO @G\n\nThought for 13 seconds >\nWhat the traceback is saying\n\n\u00a9) Copy\n\ngoogle.api_core.exceptions.NotFound: 404 ...\ngemini-pro is not found for API version vibeta, or is not supported in ge\n\nThe Google Generative Al SDK you're calling is still hitting the old \u201cv1beta\" endpoint, where the\nmodel list looked like\nmodels/gemini-pro (note the full path).\n\nThat happens when:\n\n\u20ac Work with Code Tab x\n\nMessage Bhuvan\n\n+ @\u00ae\n\nen\n\nWw\n\nS80\u00b0G8880 e270 ae\n\n.\n\n@ Wed Jul 2 10:21AM"
}