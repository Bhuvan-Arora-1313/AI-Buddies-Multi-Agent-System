{
  "timestamp": "2025-07-02_10-42-17",
  "active_window": "ChatGPT",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "print(\">>> model being used:\", model._name) ",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv(), override=True)\n\n\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nimport google.generativeai as genai\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\"))\nmodel = genai.GenerativeModel(\"models/gemini-pro\")\nprint(\">>> model being used:\", model._name) \n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    full_prompt = f\"{SYSTEM_PROMPT}\\nScenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    out = json.loads(model.generate_content(full_prompt).text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]",
  "ocr_text": "@ ChatGPT File Edit View\n\nWindow Help\n\n6 \u00a30 OS mH FQ\n\n@ Wed Jul 2 10:42AM\n\n= List of research 5\n\nmc > Cf\nKS)\n| EXPLORER\n\n> __pycache__\n\n\u00a9\n\nPp v venv\n\n\u00bb > bin\n> include\n\nS > lib\n\nPS\n\nA\n\n\u00a9 pyvenv.cfg\n .env\n\n@ excuse_api.py\n\n{} history.json\n\n= requirements.txt\n\n@\n$03 > OUTLINE\n\n> TIMELINE\n\nGE & maine @0A4\n\n\\Y EXCUSE-GENERATOR\n\n@ Artificial Intellige CO NLP_1.ipynb - Co\n\nO @& = aistudio.google.com/apikey\n\ne=> PP excuse-generator\nx Welcome @ excuse_api.py 4,U @\u00ae % .env U\n@ excuse_api.py > ...\n7\n8\n\n9 from fastapi import FastAPI\n10 from pydantic import BaseModel\n11\n\nCO NLP_1.ipynb - Co\n\nca LAUNCHED Glob M Launched - Artif Artificial_Intelligence\n\n8-~ eo\nDy &\n\n12 import google. generativeai as genai\n13 genai.configure(api_key=os.getenv(\"GEMINI_KEY\") )\n\n14 model\n\n15 print(\">>> model being used:\", model.\n16 # ---------- simple \u201cDB\u201d ------\n\n17 DATA = Path(\"history. json\")\n\n18 if not DATA.exists():\n\n19 DATA.write_text(\"[]\")\n\n20\n\n21\n\nPROBLEMS @ OUTPUT DEBUGCONSOLE TERMINAL PORTS.\n187, in __call__\nraise exc\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.\n\n165, in __call__\nawait self.app(scope, receive, _send)\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.\n\nline 62, in __call__\nawait wrap_app_handling_exceptions(self.app, conn) (scope,\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.\n\ne 53, in wrapped_app\nraise exc\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.\n\ne 42, in wrapped_app\nawait app(scope, receive, sender)\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.\n\ncall__\nawait self.middleware_stack(scope, receive, send)\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.\n\npp\nawait route.handle(scope, receive, send)\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.\n\nandle\nawait self.app(scope, receive, send)\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.\n\nfile.pdf ITRV (1).pdf ITRV.pdf\n\n= genai.GenerativeModel(\"models/gemini-pro\")\n\n_name)\n\n# seed empty list\n\n11/site-packages/starlette/middleware/errors.py\", line\u201d\nt\n\n11/site-packages/starlette/middleware/exceptions.py\",\n\nreceive, send) '\n11/site-packages/starlette/_exception_handler.py\", eho\n\n11/site-packages/starlette/_exception_handler.py\", lin\nt\n\n11/site-packages/starlette/routing.py\", line 714, in a\n\n11/site-packages/starlette/routing.py\", line 734, in asl\nt\n\n11/site-packages/starlette/routing.py\", line 288, in h\n\nt\n\nae \u2014packages/starlette/routing.py\", line 76, in ap\nLn15,Col45 Spaces:4 UTF-8 LF {} Python 8&3 3.13.264-bit (\n\nAIEUTIR EAI EY Iko to MTJ (1).pdf\n\nIko to mtj\n\n(1B al-05-BBLEN4\n\n\u00a9 Openal o API keys - Open/ (G) Gemini API refer:\n\n96 Get API key | GX\n\neco MO @G\n\nIko to MTJ--=\n\nIntelligent Excuse Generator Plan >\nBhuvan\n\n2 \u2014 Make the assignment bullet-proof\n\nSometimes an older import elsewhere in the file executes first.\nPut the Gemini block immediately after load_dotenv, before any other\nimports:\n\npython\n\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv(), override=True)\n\nimport google.generativeai as genai\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\") )\nmodel = genai.GenerativeModel(\"models/gemini-pro\")\n\nEnsure there is no second import google.generativeai later in the file.\n\n3 \u2014 Restart cleanly\n\nbash\n\npkill -f \"uvicorn.*excuse_api\" 2>/dev/null\nWatch for the temporary >>> model being used: line; confirm it shows\nmodels/gemini-pro.\n\n\u00a9 excuse_api.py x\n\nMessage Bhuvan\n\n+ @ &\n\n980\u00b0G8820 \u00a9982\u00b00 Ef\n\n\u00a9) Copy\n\n\u00a9) Copy\n\n=]"
}