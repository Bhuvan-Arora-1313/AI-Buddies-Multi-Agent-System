{
  "timestamp": "2025-07-02_10-20-26",
  "active_window": "Electron",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "out = json.loads(model.generate_content(full_prompt).text)",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nload_dotenv()          # \u2190 NEW: pulls OPENAI_API_KEY from .env\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nimport google.generativeai as genai\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\"))\nmodel = genai.GenerativeModel(\"gemini-pro\")\n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    full_prompt = f\"{SYSTEM_PROMPT}\\n{prompt}\"\n    out = json.loads(model.generate_content(full_prompt).text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]",
  "ocr_text": "@ Code File Edit Selection View Go Run Terminal Window Help S6\u00a9 \u20ac8 \u00a9GD O S 2 F Q S \u00a9 Wed Jul2 10:20AM\n= List of research f @ Artificial Intellige CO NLP_1.ipynb - Co CO NLP_1.ipynb - Co ca LAUNCHED Glob M Launched - Artif Artificial_Intelligence | Al-05-BBLEN4 \u00a9 Openal o API keys - Openf (G) Gg\nMaccy\nAg out =\nO<\u00abvr Ca O 8 & aistudio.google.com/apikey eee OG Intelligent Excuse Generator Plan > @ json.loads(model.generate_content(full_pro\nBhuvan mpt).text)\nc5oe3e & > PP excuse-generator By ee * .\nS \u00b0 4 Edit excuse_api.py ~\no | EXPLORER J Welcome \u00ae excuse_api.py 6,U @ %% env. U aan\n= \\ EXCUSE-GENERATOR \u00ae excuse_api.py > \u00a9 generate 9 woe\nio) > \u2014pycache 33 Replace the OpenAl block with Gemini.\nan 0 Pay a 1 . .\n% a 34. # \u2014\u2014\u2014\u2014-\u2014- /generate ---------- Here's the minimal diff (copy exactly what's between the lines):\n1 u iT} Bien P|\n> include 35 @app.post(\"/generate\") diff \u00ae Copy\nb\u00ae PB 36 def generate(r: Req):\n& \u00a9 pyvenv.cfg U n . . n\nca u 37 prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency} ee -\nFS @ excuse_apipy 6, 38 full_prompt = f\"{SYSTEM_PROMPT}\\n{prompt}\" -from openai import OpenAl\nainictontisen 5 39 -client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\") )\nSpicmeninek 0 +import google.generativeai as genai\n40 entry = { +genai.configure(api_key=os.getenv(\"GEMINI_KEY\") )\ny\n41 \"id\": hashlib.md5 (out [\"excuse\"].encode()).hexdigest(), a a ea\n42 \"ts\": time.time(),\nand further down, swap the LLM call:\n43 **\u00abOUT , |\nee |\n44 } 1 diff & Copy\n45\n46 history = json. loads(DATA. read_text() ) - res = client.chat.completions.create(\n47 if entry[\"id\"] not in {h[\"id\"] for h in history}: # de-dupe 2 Lie '\n. aa messages=\n48 history. append(entry) 7 {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n49 DATA.write_text(json.dumps(history, indent=2) ) = {\"role\": \"user\", \"content\": prompt},\n50 F 1,\n- temperature=0.8,\n51 return entry = )\n52 = out = json.loads(res.choices[0@].message.content)\n+ full_prompt = f\"{SYSTEM_PROMPT}\\n{prompt}\"\nee /top?n=5 ---------- i aoe = Seas\n5A fann aat(\"/+tan\")\nPROBLEMS @ OUTPUT DEBUGCONSOLE TERMINAL PORTS +y A x . . .\n(Everything else\u2014history file, FastAPI routes\u2014stays the same.)\nUsing cached httplib2-0.22.0-py3-none-any.whl (96 kB) aa) BJzsh\nUsing cached pyparsing-3.2.3-py3-none-any.whl (111 kB) >] zsh\nUsing cached uritemplate-4.2.0-py3\u2014-none-any.whl (11 kB) ~\nInstalling collected packages: urllib3, uritemplate, pyparsing, pyasn1, protobuf, grpcio, charset_normalizer, celica Save the file\nools, rsa, requests, pyasni-modules, proto-plus, httplib2, googleapis\u2014-common-protos, grpcio-status, google-auth, goon . |\n@ gle-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\nSuccessfully installed cachetools-5.5.2 charset_normalizer-3.4.2 google-ai-generativelanguage-0.6.15 google-api-corey a\n-2.25.1 google-api-python-client-2.174.@ google-auth-2.4@.3 google-auth-httplib2-0.2.@ google-generativeai-0@.8.5 goo\n> OUTLINE gleapis\u2014common-protos-1.70.@ grpcio-1.73.1 grpcio-status-1.71.2 httplib2-0.22.@ proto-plus-1.26.1 protobuf-5.29.5 Py, v\n$03 asn1-@.6.1 pyasni-modules-0.4.2 pyparsing-3.2.3 requests-2.32.4 rsa-4.9.1 uritemplate-4.2.@ urllib3-2.5.@\n> TIMELINE (.venv) bhuvanarora@Bhuvans-MacBook-Pro excuse-generator % []\nwy % main* \u00ae@ @o0A6 Q_ Ln39,Col1 Spaces:4 UTF-8 LF {} Python @ 3.13.264-bit\n\u2014 = 1 _\u2014 \u2018 Message Bhuvan\n+ @& \u00a2 @\noy file.pdf ITRV (1).pdf ITRV.pdf JETIR2101184 Iko to mtj Iko to MTJ (1).pdf = Iko to MTJ-s.\n\n92068820 eee"
}