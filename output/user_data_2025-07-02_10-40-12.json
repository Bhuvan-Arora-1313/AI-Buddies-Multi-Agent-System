{
  "timestamp": "2025-07-02_10-40-12",
  "active_window": "ChatGPT",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "python - <<'PY'\nimport os, dotenv, google.generativeai as genai, pathlib, sys\ndotenv.load_dotenv('.env', override=True)\n\nprint(\"SDK version  :\", genai.__version__)\nprint(\"API key prefix:\", os.getenv(\"GEMINI_KEY\")[:10] if os.getenv(\"GEMINI_KEY\") else \"MISSING\")\n\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\"))\ntry:\n    print(\"\\nModels visible to this key:\")\n    for m in genai.list_models():\n        print(\"  \u2022\", m.name)\nexcept Exception as e:\n    print(\"\\n\u26a0\ufe0f  list_models() raised:\", e.__class__.__name__, e)\n    sys.exit(1)\nPY",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv(), override=True)\n\n\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nimport google.generativeai as genai\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\"))\nmodel = genai.GenerativeModel(\"models/gemini-pro\")\n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    full_prompt = f\"{SYSTEM_PROMPT}\\nScenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    out = json.loads(model.generate_content(full_prompt).text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]",
  "ocr_text": "ChatGPT File Edit View Window Help . mt) F Q BS Wed Jul 2 10:40AM\n+\n\n= z co co ca M 3] iS) 6 \u00a9 96 Get API key | GX\n, , F . __\nO-<e 7Coea O & & aistudio.google.com/apikey eee OG Intelligent Excuse Generator Plan > @ =\nBhuvan\n6 \u20ac PP excuse-generator By ib OL 7 \u2014\n2 | oO EXPLORER J Welcome \u00ae excuse_api.py 4,U X %%.env U Py %& O\n\\ EXCUSE-GENERATOR \u00ae excuse_api.py > \u00a9 generate\n) P > __pycache__ # ---------- /gene rate \u2014--------- =\nVv env u hn ES\n\u00ae 5 bin @app.post(\"/generate\")\n> include PROBLEMS @ OUTPUT DEBUGCONSOLE TERMINAL PORTS. tye a x\n> >t >-| zsh\nbe \u00a9 pyvenv.cf U SDK version : 0.8.5 i 2\n5 i s 5 APL Key prefix: AlzasyCMsb x] zsh Thought for 22 seconds >\nH ] e i. 4,U Models visible to this key: = i it i i :\nESN _STIH) Elfen vsterad oon Great \u2014 your diagnostic confirms everything we need\n{} history.json U * models/gemini-1.0\u2014-pro-vision-latest [\n= : ts.txt U * models/gemini-pro-vision 9 ion:\nA requirements.tx : We See SDK version: 0.8.5 (new enough)\n* models/gemini-1.5-pro-@02\n* models/gemin \u2014pro .\n* models/gemin \u2014-flash-latest I, \u00b0 Key loaded: good\n* models/gemini-1.5-flash\n* models/gemini-1.5-flash-002 a9 4 Pay : .\n+ models/gemini-1.5-flash-8b + models/gemini-pro is visible in the list\n* models/gemini-1.5-flash-8b-001\n* models/gemini-1.5-flash-8b-latest\n* models/gemini-2.5-pro-preview-03-25\n* models/gemini-2.5-flash-preview-04-17 1 So access is fine.\n. po oe eee as review-05-20\n* models/gemini-2.5-flas i i \u2018 A F\n nodels/gemini_2.5-flash-preview-04-17-thinking : The 404 is only because the server is still calling the model with\n* models/gemin -f lash-lite-preview-06-17 th \" eae \"\ni i e short name (\"gemini-pro\") rather than the full path\n* models/gemini-2.5-pro-preview-05-06\n* model ini-2.5-pro-preview-06-05 +e . ai\neS eS eee \"models/gemini-pro\" (or the old code is still in memory).\n* models/gemini-2.@-flash-exp\n* models/gemini-2.0-flash\n* models/gemini-2.0-flash-001 F . .\n+ models/gemini-2.0-flash-exp-image-generation Do these two tiny fixes and you'll be done:\n* models/gemini-2.0-flash-lite-001\n* models/gemini-2.0-flash-lite Ho\n* models/gemin -f lash-preview-image-generation\n* models/gemin -f lash-Lite-preview-02-05\n. pore semte aan aad\n* models/gemini-2.@\u2014-pro-exp\n+ models/gemini-2.0-pro-exp-02-05 1 Make sure the code uses the full model name\n* models/gemini\u2014exp-1206\n* models/gemini-2.0-flash-thinking-exp-@1-21 at\n* models/gemini-2.0-f lash-thinking-exp\n* models/gemini-2.0-flash-thinking-exp-1219 . .\n* models/gemini-2.5-flash-preview-tts if At the very top of excuse_ap1.py keep exactly:\n* models/gemini-2.5-pro-preview-tts\n* models/learn\\m-2.0-flash-experimental ra) Q\n* models/gemma-3-1b-it hon Co\n* models/gemma-3-4b-it pyt Py\n()) * models/gemma-3-12b-it v\n* models/gemma-3-27b-it\n* models/gemma-3n-e4b-it rf\n> OUTLINE * models/gemma-3n-e2b-it\n$03 * models/embedding-001 A\n> TIMELINE * models/text-embedding-004 6 excuse_api.py x\nWE & maine @0A4 Q 1n42,Col63 Spaces:4 UTF-8 LF {} Python @ 3.13.264-bit\n\u2014 \u2014 ~~ Message Bhuvan\nia)\n= + @& a)\noy file.pdf ITRV (1).pdf ITRV.pdf JETIR2101184 Iko to mtj Iko to MTJ (1).pdf = Iko to MTJ-s..\n\nS800eEGE80 \u00a98210 &*"
}