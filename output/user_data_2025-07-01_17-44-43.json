{
  "timestamp": "2025-07-01_17-44-43",
  "active_window": "firefox",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "git push",
  "vscode_text": "<!DOCTYPE html>\n<html>\n  <body style=\"margin: 0; background: rgba(0,0,0,0.7); color: white; font-family: sans-serif;\">\n    <div style=\"-webkit-app-region: drag; padding: 10px; background: rgba(0,0,0,0.8);\">\n      <pre id=\"output\" style=\"white-space: pre-wrap; word-wrap: break-word;\">Waiting for data...</pre>\n    </div>\n    <script>\n      const { ipcRenderer } = require('electron');\n      ipcRenderer.on('update-prediction', (event, data) => {\n        document.getElementById('output').textContent = data;\n      });\n    </script>\n  </body>\n</html>",
  "ocr_text": "=} List of research papers - Google X N_ Enhanced multi agent coordinat X\n\nC Oo OD ffile:///Users/bhuvanarora/Downloads/s41598-025-88145-7-2.pdf\n\nY ChatGPT OX\n\n=O ChatGPT v G\n\nSaved memory full \u00a9\n\n\u00ae When Drones Stay Closer\n(Like a Flock):\ne Purpose: Cooperate tightly for a goal.\ne Examples:\ne Light shows\nCargo transport in groups\n\nDefense swarms attacking together\nIn these cases, moving together like\n\nbirds is useful.\n\n\u00bb Key Point:\n\nSwarm behavior + Always flying close.\nIt means they coordinate and cooperate.\nSometimes they spread out, sometimes they\n\nfly together\u2014it depends on the mission.\n\nDo you want me to explain how drones stay\n\nconnected when the \\V \u2018e dispersed?\ni a rs RK\n\nAsk anything\n\n+ 2 Tools Q\n\nChatGPT can make mistakes. Check important info. See Cookie\nPreferences.\n\n* 541598-025-88145-7-2.pdf \u00a9) JayKap-Git/Buday\n\n\u2014 + 160% v\nvalue of the radius and angle use TanH as the activation function.\n\nHighlight colour\nIn this study, the navigator is the point mass consisting of the position and 1 C) ee @\nthe virtual navigator to approach the target area; the swarm can avoid obstac\nnavigator to reduce the distance between the navigator and swarm\u201d\u00b0. The consimublarss\nis as follows: s\n\nG= Gobstacle + Gleader + Geenter nn\nShow all [ @)\nIn Formula (23), Gieader rewards the leader for getting closer to the destinatiOM, ana Urcenter eward\n\nthe cluster center for shrinking the distance from the leader. The distance between the navigators Gobstacle\nis rewards the cluster center for avoiding obstacles. Its network input is the navigator\u2019s position, the drone\ncluster\u2019s center position. The distance vector between it and the obstacle, and the output is the velocity vector\nof the navigator. In order to achieve cooperative behavior of drone clusters, this study improved the multi-\nagent control strategy. Distributed control is achieved between drones through local communication, avoiding\nexcessive reliance on centralized control. It also dynamically adjusts parameters by calculating the relative\ndistance and speed difference between agents to ensure that the cluster maintains an appropriate formation\nduring patrols. Finally, based on the group behavior model of Olfati-Saber, it introduces the gradient descent\nmethod to dynamically adjust the flight path to avoid collisions between drones and obstacles. By combining\nthe DQN and PPO algorithms, this study uses the target network and experience replay mechanism to reduce\nthe instability of model training, and uses parallel computing to accelerate the execution of path planning and\nobstacle avoidance strategies. It also uses the Bayesian optimization method to dynamically adjust parameters\nsuch as learning rate and discount factor to improve the robustness of the algorithm in complex environments.\n\nEnhanced multi-agent cluster model construction\n\nBefore building the model, this study set the learning rates of the DQN and PPO algorithms to 0.001 and\n0.0003. The learning rate of DQN is mainly based on the convergence requirements of the algorithm in\na dynamic environment, and a higher learning rate is selected to accelerate initial learning. The learning\nrate of PPO is based on empirical values and experimental tuning results, and a lower learning rate is\nselected to avoid oscillation or instability caused by too fast gradient updates in complex scenarios.\nThis study also set the batch size of DQN and PPO to 64 and 128. The smaller batch size of DQN is\nintended to improve the real-time update capability of the model, while the larger batch size of PPO helps\nto enhance the adaptability to complex environments and training stability. The discount factor is used\nto weigh short-term rewards and long-term benefits. This study set it to 0.99, highlighting the overall\nperformance of the drone swarm in long-term patrol missions and retaining the effectiveness of short-\nterm path optimization. The target network of the DQN algorithm is updated every 100 iterations. Based\non the consideration of model training stability, it delays the update of the target network to avoid training\noscillations caused by frequent updates. The PPO algorithm uses gradient clipping technology and sets the\ngradient clipping threshold to 0.2 to ensure that the gradient update does not exceed a reasonable range,\nthereby improving the stability of the model.\n\nIn order to verify the effectiveness and correctness of the algorithm, this study also built a multi- drone motion\nsimulation model and uses the four-degree-of-freedom 8-state drone dynamics model*\u2019. It contains 8 state\nvariables [s1, s2, s3, s4, s5, s6, s7, s8] and four input variables: the aileron deflection command Qa, elevator\nand rudder deflection command @y, rudder angle command 9, and throttle command Qa. In order to control"
}