{
  "timestamp": "2025-07-02_11-17-18",
  "active_window": "ChatGPT",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "# Intelligent Excuse Generator\n\nFastAPI micro-service that creates believable excuses plus a chat-log \u201cproof,\u201d\nstores history, ranks past excuses, and simulates an emergency text\u2014powered by\nGoogle Gemini (via LangChain).\n\n## 1 . Setup\n```bash\ngit clone https://github.com/<yourUser>/excuse-generator.git\ncd excuse-generator\npython -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv(), override=True)\n\n\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\n# --- Gemini via LangChain (REST v1) ---\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.schema import SystemMessage, HumanMessage\n\n# expose key for LangChain wrapper\nos.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")  # must be in .env\n\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",          # free\u2011tier model\n    temperature=0.8,\n    convert_system_message_to_human=True\n)\n#print(\">>> model being used:\", model._name) \n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n\nclass EmergencyRequest(BaseModel):\n    number: str\n    message: str\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    full_prompt = f\"{SYSTEM_PROMPT}\\nScenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    messages = [\n        SystemMessage(content=SYSTEM_PROMPT),\n        HumanMessage(content=full_prompt)\n    ]\n    response_text = llm(messages).content.strip()\n    # LangChain may wrap JSON in ``` blocks \u2013 strip them\n    if response_text.startswith(\"```\"):\n        response_text = response_text.strip(\"`\").lstrip(\"json\").strip()\n    out = json.loads(response_text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]\n\n# ---------- /emergency ----------\n@app.post(\"/emergency\")\ndef emergency(req: EmergencyRequest):\n    \"\"\"\n    Simulate sending an emergency SMS/call.\n    For the demo we just log the request and append an entry to history.json.\n    \"\"\"\n    entry = {\n        \"id\": f\"emergency-{int(time.time())}\",\n        \"ts\": time.time(),\n        \"excuse\": \"EMERGENCY TRIGGER\",\n        \"believability_score\": 1.0,\n        \"chat_log\": f\"Sent '{req.message}' to {req.number}\"\n    }\n    history = json.loads(DATA.read_text())\n    history.append(entry)\n    DATA.write_text(json.dumps(history, indent=2))\n    return {\"status\": \"sent\", \"to\": req.number, \"msg\": req.message}",
  "ocr_text": "@ ChatGPT File\n\nEdit View\n\nWindow\n\nHelp\n\n\u00a9 fF OO 8\n\n60%) FQ BS\n\n@ Wed Jul 2 11:17AM\n\no\nS\n\n\u00a9\n\n= List of research 5\n\n\u20ac > C\n\nGoogle Al Studio\n\n| o EXPLORER\nV EXCUSE-GENERATOR\n> __pycache__\nv venv\n> bin\n> include\n> lib\n\u00a9 pyvenv.cfg\n .env\n\n@ excuse_api.py ty\n{} history.json\n\n@ README.md\n\n= requirements.txt\n\n=> BY & oO\n\n@ Artificial Intellige CO NLP_1.ipynb - Co CO NLP_1.ipynb - Co ca LAUNCHED Glob\n\nM Launched - Artif\n\naistu\u00a2-\u2014\u2014\u2014\u2014+\n\nO 6 =\n\nria | PyCharm Qs ct\nCe \u00a3 excuse-generator By Ean WwW\n@ excuse_api.py5 X  {} history.json @ README.md & .env U By\n\n\u00ae excuse_api.py > @ emergency\n\n87 def emergency(req: EmergencyRequest) :\n\n100 history.append(entry)\n\n101 DATA.write_text(json.dumps(history, indent=2) ) =\n102 return {\"status\": \"sent\", \"to\": req.number, \"msg\": req.message} ~~\nPROBLEMS @ OUTPUT DEBUGCONSOLE TERMINAL PORTS +v A\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in | vJ zsh\nimport_from_string an bJzsh\n\nmodule = importlib. import_module(module_str)\n\nFile \"/Users/bhuvanarora/.pyenv/versions/3.11.13/lib/python3.11/importlib/__init__.py\", line 126, in import_mo,\u00ae\n\ndule\nreturn _bootstrap._gcd_import(name[level:], package, level)\nFile \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\nFile \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\nFile \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n\nFile \"<frozen importlib._bootstrap>\", line 69@, in _load_unlocked\n\nFile \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n\nFile \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n\nFile \"/Users/bhuvanarora/excuse-generator/excuse_api.py\", line 17, in <module>\nos.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") # must be in .env\n\nFile \"<frozen os>\", line 684, in __setitem__\nFile \"<frozen os>\", line 758, in encode\n@ TypeError: str expected, not NoneType\nACINFO: Stopping reloader process [6527]\n(.venv) bhuvanarora@Bhuvans\u2014MacBook-Pro excuse-generator % uvicorn excuse_api:app --reload\n\nINFO: Will watch for changes in these directories: ['/Users/bhuvanarora/excuse-generator']\nINFO Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n\nINFO Started reloader process [6653] using StatReload\n\nINFO Started server process [6655]\n\nINFO Waiting for application startup.\n\nINFO Application startup complete.\n\n/Users/bhuvanarora/excuse-generator/excuse_api.py:56: LangChainDeprecationWarning: The method \u2018BaseChatModel.\n\nall_ was deprecated in langchain-core @.1.7 and will be removed in 1.0. Use :meth:*~invoke* instead.\nresponse_text = 11m(messages).content.strip()\n\n/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-packages/langchain_google_genai/chat_models. py: 483\n\n: UserWarning: Convert_system_message_to_human will be deprecated!\nwarnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n\nINFO: 127.0.@.1:65321 - \"POST /generate HTTP/1.1\" 200 OK\nACINFO: Shutting down (\nINFO: Waiting for application shutdown.\n\n(2) INFO:\nINFO:\nINFO:\n\n$03 > OUTLINE\n\n> TIMELINE\n\n| maint @\u00ae @O0AS5\nView status\n\nApplication shutdown complete.\nFinished server process [6655]\nStopping reloader process [6653] }\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % pip freeze > requirements.txt\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % nano README.md\n(.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % o\n\nQ_ 1n102,Col68 Spaces:4 UTF-8 LF\n\n. . . . .\n\n{} Python\n\nBy\n\n.\n\nArtificial_Intelligence\n\nNew |!\n\n: j\nSB 3.13.2 64-bit OO\n\nSere\n\n.\n\n| Al-05-BBLEN4 \u00a9 Openal o API keys - Open/ (G) Gemini API refer: * Get API key | G X\neee M7 ZG intelligent Excuse Generator Plan > Ee\nhistory.json. Review | @ Revert\n91 won\n92 entry = {\n93 \"id\": f\"emergency\u2014{int(time.time())}\",\n94 \"ts\": time.time(),\n95 \"excuse\": \"EMERGENCY TRIGGER\",\n96 \"believability_score\": 1.0,\n97 \"chat_log\": f\"Sent '{req.message}' to {req.number}\"\n98 }\n99 history = json.loads(DATA.read_text())\n100 history.append(entry)\n101 DATA.write_text(json.dumps(history, indent=2) )\n102 return {\"status\": \"sent\", \"to\": req.number, \"msg\": req.message}\nLooked at Code\n\nAdded an /emergency endpoint and an EmergencyRequest model:\n\n- POST /emergency expects body\n{ \"number\": \"+1123456789\",\nand returns {\"status\":\"sent\",\n\n\"message\": \"Call me now!\" }\n\n. .. } while logging the event into history. json.\n\nServer now covers the PDF's emergency-alert requirement. Restart Uvicorn and test:\nbash O) Copy\n\nuvicorn excuse_api:app -\u2014-reload\n\ncurl -X POST http://127.0.0.1:8000/emergency \\\n-H \"Content-Type: application/json\" \\\n-d '{\"number\":\"+1123456789\", \"message\":\"Call me now!\"}'\nAan 6 \u00ae@\n\n+\n\n\u00a9 excuse_api.py x\n\nMessage Bhuvan\n\n\u00ae wv\n\naiccaaesnal"
}