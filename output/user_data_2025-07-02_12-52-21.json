{
  "timestamp": "2025-07-02_12-52-21",
  "active_window": "ChatGPT",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "/Users/bhuvanarora/excuse-generator/excuse_api.py:77: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  response_text = llm(messages).content.strip()\n/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\nINFO:     127.0.0.1:49225 - \"POST /generate HTTP/1.1\" 500 Internal Server Error\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 112, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\n    return await run_in_threadpool(dependant.call, **values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/concurrency.py\", line 37, in run_in_threadpool\n    return await anyio.to_thread.run_sync(func)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/excuse_api.py\", line 89, in generate\n    from gtts import gTTS\nModuleNotFoundError: No module named 'gtts'\n",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv(), override=True)\n\n\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\n# --- Gemini via LangChain (REST v1) ---\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.schema import SystemMessage, HumanMessage\n\n# expose key for LangChain wrapper\nos.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")  # must be in .env\n\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",          # free\u2011tier model\n    temperature=0.8,\n    convert_system_message_to_human=True\n)\n#print(\">>> model being used:\", model._name) \n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n    mode: str = \"normal\"   # \"normal\" | \"apology\"\n    language: str = \"en\"   # ISO code, e.g. \"en\", \"es\", \"fr\"\n    voice: bool = False   # If true, return an MP3 of the excuse\n\nclass EmergencyRequest(BaseModel):\n    number: str\n    message: str\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    style_clause = (\n        \"Respond in a guilt\u2011tripping, heartfelt apology tone.\"\n        if r.mode.lower() == \"apology\"\n        else \"Respond in a neutral, believable tone.\"\n    )\n    # language directive\n    lang_clause = (\n        \"\" if r.language.lower() in [\"en\", \"english\"] else\n        f\"Respond in {r.language} language.\"\n    )\n    full_prompt = (\n        f\"{SYSTEM_PROMPT}\\n\"\n        f\"Tone: {style_clause}\\n\"\n        f\"{lang_clause}\\n\"\n        f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    )\n    messages = [\n        SystemMessage(content=SYSTEM_PROMPT),\n        HumanMessage(content=full_prompt)\n    ]\n    response_text = llm(messages).content.strip()\n    # LangChain may wrap JSON in ``` blocks \u2013 strip them\n    if response_text.startswith(\"```\"):\n        response_text = response_text.strip(\"`\").lstrip(\"json\").strip()\n    out = json.loads(response_text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n    # --- optional voice synthesis ---\n    if r.voice:\n        from gtts import gTTS\n        audio_dir = Path(\"audio\")\n        audio_dir.mkdir(exist_ok=True)\n        audio_file = audio_dir / f\"{entry['id']}.mp3\"\n        gTTS(out[\"excuse\"], lang=r.language[:2]).save(audio_file.as_posix())\n        entry[\"audio\"] = str(audio_file)\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]\n\n# ---------- /emergency ----------\n@app.post(\"/emergency\")\ndef emergency(req: EmergencyRequest):\n    \"\"\"\n    Simulate sending an emergency SMS/call.\n    For the demo we just log the request and append an entry to history.json.\n    \"\"\"\n    entry = {\n        \"id\": f\"emergency-{int(time.time())}\",\n        \"ts\": time.time(),\n        \"excuse\": \"EMERGENCY TRIGGER\",\n        \"believability_score\": 1.0,\n        \"chat_log\": f\"Sent '{req.message}' to {req.number}\"\n    }\n    history = json.loads(DATA.read_text())\n    history.append(entry)\n    DATA.write_text(json.dumps(history, indent=2))\n    return {\"status\": \"sent\", \"to\": req.number, \"msg\": req.message}",
  "ocr_text": "@ ChatGPT File Edit View Window Help\n\nCore Features Designed:\n\n1. Al-Generated Excuses \u2014 Context-based excuse suggestions (work, school, social, family).\n\n2. Scenario-Based Customization \u2014 Allows users to refine excuses based on urgency and\n\neS excuse-generator By\noO EXPLORER @ excuse_api.py 6 X  {} history.json M @ README.md env U\n\\ EXCUSE-GENERATOR \u00ae excuse_api.py > \u00a9 generate\n> __pycache__ Men yo aN\nVv env PROBLEMS @ OUTPUT DEBUGCONSOLE TERMINAL PORTS\n> (site raise exc P\n10K > include File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-package\n7 s/starlette/_exception_handler.py\", line 42, in wrapped_app }\n> lib await app(scope, receive, sender)\npyvenv.cfg File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-package\ns/starlette/routing.py\", line 714, in __call__\nenv U await self.middleware_stack(scope, receive, send)\n\u00ae excuse_api.py 6 File \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-package\n. 7 s/starlette/routing.py\", line 734, in app\n{} history.json M await route.handle(scope, receive, send)\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-package\ns/starlette/routing.py\", line 288, in handle\nawait self.app(scope, receive, send)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-package\ns/starlette/routing.py\", line 76, in app\nawait wrap_app_handling_exceptions(app, request) (scope, receive, send)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-package\ns/starlette/_exception_handler.py\", line 53, in wrapped_app\nraise exc\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-package\ns/starlette/_exception_handler.py\", line 42, in wrapped_app\nawait app(scope, receive, sender)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-package\ns/starlette/routing.py\", line 73, in app\nresponse = await f(request)\n\n@ README.md\n= requirements.txt\n\ni]\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-package\ns/fastapi/routing.py\", line 301, in app\nraw_response = await run_endpoint_function( I\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-package \u00ab\ns/fastapi/routing.py\", line 214, in run_endpoint_function\nreturn await run_in_threadpool(dependant.call, **values)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-package\ns/starlette/concurrency.py\", line 37, in run_in_threadpool\nreturn await anyio.to_thread. run_sync( func)\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-package\ns/anyio/to_thread.py\", line 56, in run_sync\nreturn await get_async_backend().run_sync_in_worker_thread(\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-package\ns/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\nreturn await future\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-package\ns/anyio/_backends/_asyncio.py\", line 967, in run I\nresult = context.run(func, xargs)\n\nFile \"/Users/bhuvanarora/excuse-generator/excuse_api.py\", line 89, in gener\n\nate\n> OUTLINE from gtts import gTTS\nModuleNotFoundError: No module named \u2018gtts\n> TIMELINE\nmain* \u00a9 Not Committed Yet Ln 92, Col paces: 4 UTF- LF ython\nin* S @0A6 Cc itted Q 92,Col36 \u00a7S 8 {} Pyth 8\n\n08:54\n\npo\n\nPrevious 30 Days\n\n300%\n\n\u00a9 ROO 8\n\n100% a)\n\n\u2014\n\nOo\nLe}\n\nQ @ Wed Jul 2 12:52PM\n\n] | @ iS)\nMaccy\n/Users/bhuvanarora/excuse-generator/\nv @ excuse_api.py:77:\n. LangChainDeprecationWarning: The method\necce52 M7 GG Intelligent Excuse Generator Plan > *BaseChatModel.__call__* was deprecated...\nBhuvan\n0 File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/\nOs . . .\n\u00b0 = starlette/_exception_handler.py\", line 42, in wrapped_app\nby await app(scope, receive, sender)\nEu File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/\n+y a x starlette/routing.py\", line 73, in app\n\"| BI pythons. Show full message\n>.) zsh\n@\nt\nt\n\u00a9 excuse_api.py x\nMessage Bhuvan\n3.13.2 64-bit O\n234.3 MB\n+ \u00ae v& 0\nSE2eC \u00a98008 \u00a3*"
}