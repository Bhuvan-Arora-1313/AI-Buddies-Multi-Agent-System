{
  "timestamp": "2025-07-02_10-41-27",
  "active_window": "ChatGPT",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "python - <<'PY'\nimport os, dotenv, google.generativeai as genai, pathlib, sys\ndotenv.load_dotenv('.env', override=True)\n\nprint(\"SDK version  :\", genai.__version__)\nprint(\"API key prefix:\", os.getenv(\"GEMINI_KEY\")[:10] if os.getenv(\"GEMINI_KEY\") else \"MISSING\")\n\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\"))\ntry:\n    print(\"\\nModels visible to this key:\")\n    for m in genai.list_models():\n        print(\"  \u2022\", m.name)\nexcept Exception as e:\n    print(\"\\n\u26a0\ufe0f  list_models() raised:\", e.__class__.__name__, e)\n    sys.exit(1)\nPY",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv(), override=True)\n\n\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nimport google.generativeai as genai\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\"))\nmodel = genai.GenerativeModel(\"models/gemini-pro\")\n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    full_prompt = f\"{SYSTEM_PROMPT}\\nScenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    out = json.loads(model.generate_content(full_prompt).text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]",
  "ocr_text": "6\n\no\nS\n\n\u00a9\n\nChatGPT File Edit View Window\u2019 Help\n\n6 \u20ac8 OGD O S wt F Q S \u00a9 Wed Jul2 10:41AM\n\n= List of research # Artificial Intellige CO NLP_1.ipynb - Cc CO NLP_1.ipynb - Cc ca LAUNCHED Glok M Launched - Artif Artificial_Intelligence B Al-05-BBLEN4 \u00a9 Openal o API ke Ope (S) Gemini API refer * Get API key | G X\neC > GC \u00ae@ O 8 & aistudio.google.com/apikey eee | G@ _ Mtelligent Excuse Generator Plan > C\nBhuvan\n< p excuse-generator 8 a Os i (a) a Www oO WV\n| \u2018a EXPLORER + x) Welcome \u00ae excuse_apipy 4,U X env U Pru oD let me tell u that i had already updated it to model/gemini-\n\\ EXCUSE-GENERATOR \u00ae excuse_api.py > \u00a9 generate pro vefore the error occured\nPP > \u2014Byeache_ 37. # ---------- /generate ----------\nVv .venv =\nu iT} i\nO\u00b0 38 @app.post(\"/generate\") i\n> include PROBLEMS @ OUTPUT DEBUGCONSOLE TERMINAL PORTS. tye A xX\nS > lib 5\n3 & pyvenv.cfg models/gemini-1.5-flash-002 i | J zsh I'm trying to figure out why a 404 error keeps happening despite using the correct models/gemini-pro setup.\nmodels/gemini-1.5-flash-8b >.) zsh \u2018 \u2018 \u2018 A\n2 env node1s/gemini-1.5-flash-8b-001 Looking at the code posted, it seems to call the right environment and model. However, | noticed that the\nmodels/gemini-1.5-flash-8b-latest generate_content function might still be using a vibeta endpoint by default. It seems like the query isn\u2019t using the\n\nU\nU\nES @ excuse_api.py 4,U\n{} history.json U\n\nU\n\nA = requirements.txt\n\nmodels/aga\n\n@\n$03 > OUTLINE\n\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-exp-1206\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/learnlm-2.0-f lash-experimental :\nmodels/gemma-3-1b-it\nmodels/gemma-3-4b-it\nmodels/gemma-3-12b-it\nmodels/gemma-3-27b-it\nmodels/gemma-3n-e4b-it\nmodels/gemma-3n-e2b-it\nmodels/embedding-001 1\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\n\nmodels/imagen-3.\nmodels/imagen-4.\nmodels/imagen-4.\nmodels/veo-2.0-generate-001\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-2.\nmodels/gemini-live-2.5-flash-preview\n\n5-pro-preview-@3-25\n5-f lash-preview-04-17\n5-f lash-preview-05-20\n5-flash GOOGLE_API_USE_CLIENT_CERTIFICATE setting or something else.\n5-f lash-preview-04-17-thinking\n5-flash-lite-preview-@6-17\n5-pro-preview-@5-06\n5-pro-preview-@6-05\n\n5-pro\n\n@-flash-exp\n\n@-flash\n\nQ-flash-001\n\n@-f lash-exp-image-generation\n0-flash-lite-001 1\nQ-flash-lite\n\nQ@-f lash-preview-image-generation\nQ@-flash-lite-preview-@2-05\nQ@-flash-lite-preview\n\n@-pro-exp\n\n@-pro-exp-02-05\n\nright configuration, even though 1ist_mode1s worked fine. | need to confirm whether it's an issue with the\n\n@-f Lash-thinking-exp-1-21\n\n@-f Lash-thinking-exp\n\n0-f lash-thinking-exp-1219 if\n5-flash-preview-tts\n\n5-pro-preview-tts\n\nQ@-generate-002\n@-generate-preview-06-06\n@-ultra-generate-preview-06-06\n\n5-f lash-preview-nat ive-audio-dialog\n5-f Llash-exp-nat ive-audio-thinking-dialog\nQ-flash-live-001\n\n> TIMELINE (.venv) bhuvanarora@Bhuvans-MacBook-Pro excuse-generator % [] 6 excuse_api.py x\n\nGE & maine @0A4\n\nfile.pdf\n\nQ  n42,Col63 Spaces:4 UTF-8 LF {} Python & 3.13.264-bit (3\n\n\u2014\u2014 Message Bhuvan\n+O 0\n\nITRV (1).pdf ITRV.pdf JETIR2101184 Iko to mtj Iko to MTJ (1).pdf = Iko to MTJ-s..-\n\nS80veege20 0820 Bf"
}