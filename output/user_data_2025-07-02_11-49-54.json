{
  "timestamp": "2025-07-02_11-49-54",
  "active_window": "ChatGPT",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "curl -X POST http://127.0.0.1:8000/generate \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"scenario\":\"missed class\",\"urgency\":\"panic\",\"mode\":\"apology\"}'",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv(), override=True)\n\n\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\n# --- Gemini via LangChain (REST v1) ---\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.schema import SystemMessage, HumanMessage\n\n# expose key for LangChain wrapper\nos.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")  # must be in .env\n\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",          # free\u2011tier model\n    temperature=0.8,\n    convert_system_message_to_human=True\n)\n#print(\">>> model being used:\", model._name) \n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n    mode: str = \"normal\"   # \"normal\" | \"apology\"\n\nclass EmergencyRequest(BaseModel):\n    number: str\n    message: str\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    style_clause = (\n        \"Respond in a guilt\u2011tripping, heartfelt apology tone.\"\n        if r.mode.lower() == \"apology\"\n        else \"Respond in a neutral, believable tone.\"\n    )\n    full_prompt = (\n        f\"{SYSTEM_PROMPT}\\nTone: {style_clause}\\n\"\n        f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    )\n    messages = [\n        SystemMessage(content=SYSTEM_PROMPT),\n        HumanMessage(content=full_prompt)\n    ]\n    response_text = llm(messages).content.strip()\n    # LangChain may wrap JSON in ``` blocks \u2013 strip them\n    if response_text.startswith(\"```\"):\n        response_text = response_text.strip(\"`\").lstrip(\"json\").strip()\n    out = json.loads(response_text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]\n\n# ---------- /emergency ----------\n@app.post(\"/emergency\")\ndef emergency(req: EmergencyRequest):\n    \"\"\"\n    Simulate sending an emergency SMS/call.\n    For the demo we just log the request and append an entry to history.json.\n    \"\"\"\n    entry = {\n        \"id\": f\"emergency-{int(time.time())}\",\n        \"ts\": time.time(),\n        \"excuse\": \"EMERGENCY TRIGGER\",\n        \"believability_score\": 1.0,\n        \"chat_log\": f\"Sent '{req.message}' to {req.number}\"\n    }\n    history = json.loads(DATA.read_text())\n    history.append(entry)\n    DATA.write_text(json.dumps(history, indent=2))\n    return {\"status\": \"sent\", \"to\": req.number, \"msg\": req.message}",
  "ocr_text": "@ ChatGPT File Edit View Window\u2019 Help 6 8 \u00a90D S srg F Q SBS \u00a9 WedJul2 11:49AM\n= List of research @ Artificial Intellige CO NLP_1.ipynb - Co CO NLP_1.ipynb - Co ca LAUNCHED Glob M Launched - Artif Artificial_Intelligence | Al-05-BBLEN4 \u00a9 Openal coy API keys - Open/ (S) Gemini API refer * Get API key | GX\n\no\nS\n\n\u00a9\n\n\u20ac > C\n\nGoogle Al Studio\n\n| EXPLORER\n\n\\Y EXCUSE-GENERATOR\n\nP > __pycache__\n\nVV venv\n\n& > bin\n> include\n\n> lib\n2\n\n\u00a9 pyvenv.cfg\n\n .env\n\nHS @ excuse_api.py\n\n{} history.json\nJX @ README.md\n\n= requirements.txt\n\n@\n$03 > OUTLINE\n\n> TIMELINE\n| f maint O\nView status\n\n@0A5\n\na\n\nO @& = aistudio.google.com/apikey\n\ne=> \u00a3 excuse-generator\n\n@ excuse_api.py5 @ {} history.json @ README.md & .env U\n\u00ae excuse_api.py > @ emergency\n54 def generate(r: Req):\nf\"{SYSTEM_PROMPT}\\nTone: {style_clause}\\n\"\nf\"Scenario: {r.scenario}\\nUrgency: {r.urgency\n\na oO\nNY\n\n63; )\n64 messages = [\n65 SystemMessage(content=SYSTEM_PROMPT) ,\n\nPROBLEMS e OUTPUT DEBUG CONSOLE TERMINAL PORTS\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http://127.0.0.1:8000/gener\n-H \"Content-Type: application/json\" \\\n-d '{\"scenario\":\"missed class\",\"urgency\":\"panic\"}'\nInternal Server Errorg\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http://127.0.0.1:8000/gener\n-H \"Content-Ty; application/json\" \\\n-d '{\"scenario\":\"missed class\",\"urgency\":\"panic\"}'\nInternal Server Errorg\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http://127.0.0.1:8000/gener\n-H \"Content-Type: application/json\" \\\n-d '{\"scenario\":\"missed class\",\"urgency\":\"panic\"}'\nInternal Server Errorg\n\u00ae (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http://127.0.0.1:8000/gener\n-H \"Content-Type: application/json\" \\\n-d '{\"scenario\":\"missed class\",\"urgency\":\"panic\"}'\ncurl: (28) Failed to connect to 127.0.0.1 port 8000 after 7781 ms: Couldn't connect to server\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http://127.0.0.1:8000/gener\n-H \"Content-Type: application/json\" \\\nd '{\"scenario\":\"missed class\",\"urgency\":\"panic\"}'\n{' \"d4640a6dc6136022101f8e116987667f\", \"ts\" :1751434240.388604,\"excuse\":\"My deepest apologies for\nmedical emergency this morning \u2014 a terrifying seizure. I had to rush him to the emergency vet imme\nas a complete panic; I couldn't possibly have made it in.\",\"believability_score\":0.92,\"chat_log\":\"F\nsee you.\\nFrom Me: No, major panic. [Pet's Name] had a seizure, rushed him to emergency vet. Still\nSo sorry, hope he's okay! A\"}2\n@ (.venv) bhuvanarora@Bhuvans-MacBook-Pro excuse-generator % uvicorn excuse_api:app --reload\ncurl -X POST http://127.0.0.1:8000/emergency \\\n-H \"Content-Type: application/json\"\n-d '{\"number\":\"+1123456789\",\"\"message\":\"Call me now!\"}'\nINFO: Will watch for changes in these directories: ['/Users/bhuvanarora/excuse-generator']\nERROR: [Errno 48] Address already in use\n{\"status\":\"sent\",\"to\":\"+1123456789\",\"msg\":\"Call me now!\"}2\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http://127.0.0.1:8000/gener\n-H \"Content-Type: application/json\" \\\nd '{\"scenario\":\"missed class\", \u201curgency\":\"panic\"}'\n{\"id\" :\"\"9a03094745b90498848dac0ed1f8e20e\", \"ts\": 1751435416. 3541481,\"excuse\":\"My younger sibling had a\nush them to urgent care. It was incredibly sudden and chaotic, pure panic. I'm still at the clinic\nink about class.\",\"believability_score\":0.96,\"chat_log\":{\"Mom\":\"Hurry! They're struggling to breath|\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http://127.0.0.1:8000/emerg\nfe: application/json\" \\\n\n,\n\n9)\n\"+1123456789\", \"msg 11 me now!\" }2\n(.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % o\n\n\u00a9 Bhuvan Arora (20 minutes ago) Q\n\nRemember to use API keys securely. Don't share or embed them in public code. |\n\n. . . . . . . . .\n\nw Var os \u00a5\n\neee OG Intelligent Excuse Generator Plan >\n\nBhuvan\n\n48 class EmergencyRequest(BaseModel): Review | @@ Revert\nx 12 lines\n\n53 @app.post(\"/generate\")\n\n54 def generate(r: Req):\n\n= prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n\n= full_prompt = f\"{SYSTEM_PROMPT}\\nScenario: {r.scenario}\\nUrgency: {r.urgency}\"\n\n55 style_clause = (\n\n56 \"Respond in a guilt-tripping, heartfelt apology tone.\"\n57 if r.mode.lower() == \"apology\"\n\n58 else \"Respond in a neutral, believable tone.\"\n59 )\n\n60 full_prompt = (\n\n61 f\"{SYSTEM_PROMPT}\\nTone: {style_clause}\\n\"\n\n62 f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n63 )\n\n64 messages = [\n\n65 SystemMessage(content=SYSTEM_PROMPT) ,\nLooked at Code\n\nAdded mode field (normal/apology) and a tone-selector inside /generate.\nNow you can call:\n\nbash Y Copied\n\ncurl -X POST http://127.0.0.1:8000/generate \\\n-H \"Content-Type: application/json\" \\\n-d '{\"scenario\":\"missed class\",\"urgency\":\"panic\", \"mode\": \"apology\"}'\n\nand get a guilt-tripping apology-style excuse. =\nAmd @\n\n\u00a9 excuse_api.py x\n\nMessage Bhuvan\n\n+o@ a)"
}