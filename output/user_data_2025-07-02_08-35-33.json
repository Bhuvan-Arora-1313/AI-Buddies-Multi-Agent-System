{
  "timestamp": "2025-07-02_08-35-33",
  "active_window": "firefox",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "Smart Border Patrol Using Drones and Wireless Charging System Under Budget Limitation\n",
  "vscode_text": "<!DOCTYPE html>\n<html>\n  <body style=\"margin: 0; background: rgba(0,0,0,0.7); color: white; font-family: sans-serif;\">\n    <div style=\"-webkit-app-region: drag; padding: 10px; background: rgba(0,0,0,0.8);\">\n      <pre id=\"output\" style=\"white-space: pre-wrap; word-wrap: break-word;\">Waiting for data...</pre>\n    </div>\n    <script>\n      const { ipcRenderer } = require('electron');\n      ipcRenderer.on('update-prediction', (event, data) => {\n        document.getElementById('output').textContent = data;\n      });\n    </script>\n  </body>\n</html>",
  "ocr_text": "Firefox File Edit View History Bookmarks Tools\n\nWindow Help \u00a9 &\u00ae8 OGD O 8S se\n\n(ica) =} List of research papers - Google X A UAV patrol system using pano X * A UAV patrol system using par X Smart border patrol using drone X\n\nC a 0 8 sciencedirectassets.com\n\nnM\n\nIme\n\nv A UAV patrol system using\npanoramic stitching and\nobject detection\n\n1 Introduction\nVv 2 Related work\n2.1 Image stitching\n\n2.2 Motion ghost\nelimination\n\n2.3 Object detection\nVv 3 UAV patrol system\n\n3.1 System\narchitecture\n\n3.2 Image stitching\n\n3.3 Motion ghost\nelimination\n\nVv 3.4 Object detection\n\n3.4.1 Scene\nparsing\n\n3.4.2 Quantifying\nthe object-scene\nrelationship\n\n3.4.3 Model\nstructure\n\n4 Experiments\n\nv 4.1 Dataset and\nexperiment settings\n\n4.1.1 Dataset\n\n4.1.2 Experiments\nsettings\n\n4.2 Image stitching\nevaluation\n\n4.3 Object detection\nevaluation\n\n4.4 Results\n5 Conclusion\n\nDeclaration of\nCompeting Interest\n\nReferences\n\n+ 200%\nUU\n\nizing the ratio ns; of ns\u00a2 to total occurrences of the corresponding object class:\n\nNs t Show all\n; Tmin\n\n,n.\nVs = Dist\n\nTmax \u2014 Tmin\n\n3.4.3. Model structure\n\nIn the network structure of the Faster R-CNN approach, the object classification layer receives regions of interests (ROIs)\nand feature maps, to produce the final results which include object bounding boxes, the object category and confidence\nscores. In the initial part of this layer, ROIs from the proposed target layer and 1024 feature maps from the head network\npass through pooling layers and a fully connected layer to generate the output as illustrated in\n\nThere are two paths in this network. The first is for scoring objects with a classifier, in which cls_score_net produces\nscores for each predicted bounding box regarding each object class. These scores then pass through softmax and max func-\ntions to obtain probability values and to determine the most likely class. Meanwhile, bbox_pred_net and bbox_transform_inv\non the second path use regression coefficients to obtain more accurate predictions of bounding box locations for each object\nclass.\n\nFor the object classification score adjustment, the scene category of every predicted bounding box is determined by the\ncentral pixel\u2019s scene parsing result. Compared to calculating the mode number of pixels in the bounding box, using the\ncenter pixel only may be slightly less representative but shows higher performance (over 10 times FPS). The corresponding\nconfidence value V;; is then applied to the score with a certain weight factor f.\n\nSGO0%\u00b0eGSEE2\u00b0 O82 G\u00ae*"
}