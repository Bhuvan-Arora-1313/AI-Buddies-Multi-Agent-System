{
  "timestamp": "2025-07-01_17-57-41",
  "active_window": "firefox",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "git push",
  "vscode_text": "<!DOCTYPE html>\n<html>\n  <body style=\"margin: 0; background: rgba(0,0,0,0.7); color: white; font-family: sans-serif;\">\n    <div style=\"-webkit-app-region: drag; padding: 10px; background: rgba(0,0,0,0.8);\">\n      <pre id=\"output\" style=\"white-space: pre-wrap; word-wrap: break-word;\">Waiting for data...</pre>\n    </div>\n    <script>\n      const { ipcRenderer } = require('electron');\n      ipcRenderer.on('update-prediction', (event, data) => {\n        document.getElementById('output').textContent = data;\n      });\n    </script>\n  </body>\n</html>",
  "ocr_text": "=} List of research papers - Google X\n\nC a\n\nY ChatGPT OX\n\n=o ChatGPT v G\nSaved memory full \u00a9\nPF Why \"Proximal\"?\ne \"Proximal\" means staying close to the\nold policy.\n\nPPO limits the amount of change to keep\n\nlearning safe and stable.\n\n\u00ae How PPO Works (Simple\nSteps):\n1. The agent tries actions and collects\nexperience (like rewards or penalties).\n\n. The agent updates its policy but keeps\nthe changes small using a \"clipping\"\n\nmethod.\n\n. Repeat: Try > Learn > Update > Try\nAgain.\n\nWhere PPO is Used:\n\ne Games (like bea humans in strategy\n\ngames)\n\nAsk anything\n\n+ 2 Tools Q\n\nChatGPT can make mistakes. Check important info. See Cookie\n63 Preferences.\n\nN_ Enhanced multi agent coordinat X\n\n&\n\n* s41598-025-88145-7-2.pdf\n\naw\n\nVv\n\n9 of 17\n\nx\n\n\u00a9) JayKap-Git/Buday\n\nOD ffile:///Users/bhuvanarora/Downloads/s41598-025-88145-7-2.pdf\n\n\u2014 + 160%\nvelocity vector. The output of the Actor-network is assumed as the radius of the balligieuleiscorats\nthe angle between the sphere radius o and the z-axis, is the angle between the rz C) ee @\nx, y and the axis x, which obtains the velocity vector [G cost, Gsintsinpw, Gs\nthe drone\u2019s speed o will be limited to [30.0 m, 50.0 m], The angle is limited to [\u201471/iiiigares\nvalue of the radius and angle use TanH as the activation function. e\n\nIn this study, the navigator is the point mass consisting of the position and\nthe virtual navigator to approach the target area; the swarm can avoid obstacle\nnavigator to reduce the distance between the navigator and swarm\u201d\u00b0. The cons\nis as follows:\n\n| @)\n\nG= Gobstacle + Gleader + Geenter (23)\n\nIn Formula (23), Gieader rewards the leader for getting closer to the destination, and Gceenter is rewards\nthe cluster center for shrinking the distance from the leader. The distance between the navigators Gobstacle\nis rewards the cluster center for avoiding obstacles. Its network input is the navigator\u2019s position, the drone\ncluster\u2019s center position. The distance vector between it and the obstacle, and the output is the velocity vector\nof the navigator. In order to achieve cooperative behavior of drone clusters, this study improved the multi-\nagent control strategy. Distributed control is achieved between drones through local communication, avoiding\nexcessive reliance on centralized control. It also dynamically adjusts parameters by calculating the relative\ndistance and speed difference between agents to ensure that the cluster maintains an appropriate formation\nduring patrols. Finally, based on the group behavior model of Olfati-Saber, it introduces the gradient descent\nmethod to dynamically adjust the flight path to avoid collisions between drones and obstacles. By combining\nthe DQN and PPO algorithms, this study uses the target network and experience replay mechanism to reduce\nthe instability of model training, and uses parallel computing to accelerate the execution of path planning and\nobstacle avoidance strategies. It also uses the Bayesian optimization method to dynamically adjust parameters\nsuch as learning rate and discount factor to improve the robustness of the algorithm in complex environments.\n\nEnhanced multi-agent cluster model construction\n\nBefore building the model, this study set the learning rates of the DQN and PPO algorithms to 0.001 and\n0.0003. The learning rate of DQN is mainly based on the convergence requirements of the algorithm in\na dynamic environment, and a higher learning rate is selected to accelerate initial learning. The learning\nrate of PPO is based on empirical values and experimental tuning results, and a lower learning rate is\nselected to avoid oscillation or instability caused by too fast gradient updates in complex scenarios.\nThis study also set the batch size of DQN and PPO to 64 and 128. The smaller batch size of DQN is\nintended to improve the real-time update capability of the model, while the larger batch size of PPO helps\nto enhance the adaptability to complex environments and training stability. The discount factor is used\nto weigh short-term rewards and long-term benefits. This study set it to 0.99, highlighting the overall\nperformance of the drone swarm in long-term patrol missions and retaining the effectiveness of short-\nterm path optimization. The target network of the DQN algorithm is updated every 100 iterations. Based\non the consideration of model training stability, it delays the update of the target network to avoid training\noscillations caused by frequent updates. The PPO algorithm uses gradient clipping technology and sets the\ngradient clipping threshold to 0.2 to ensure that the gradient update does not exceed a reasonable range,\nthereby improving the stability of the model."
}