{
  "timestamp": "2025-07-02_10-58-12",
  "active_window": "Electron",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "curl -X POST http://127.0.0.1:8000/generate \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"scenario\":\"missed class\",\"urgency\":\"panic\"}'",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv(), override=True)\n\n\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\n# --- Gemini via LangChain (REST v1) ---\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.schema import SystemMessage, HumanMessage\n\n# expose key for LangChain wrapper\nos.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")  # must be in .env\n\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",          # free\u2011tier model\n    temperature=0.8,\n    convert_system_message_to_human=True\n)\n#print(\">>> model being used:\", model._name) \n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    full_prompt = f\"{SYSTEM_PROMPT}\\nScenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    messages = [\n        SystemMessage(content=SYSTEM_PROMPT),\n        HumanMessage(content=full_prompt)\n    ]\n    response_text = llm(messages).content.strip()\n    # LangChain may wrap JSON in ``` blocks \u2013 strip them\n    if response_text.startswith(\"```\"):\n        response_text = response_text.strip(\"`\").lstrip(\"json\").strip()\n    out = json.loads(response_text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]",
  "ocr_text": "@ Code File Edit Selection\n\nView Go  Run_ Terminal Window\u2019 Help\n\n6 8 \u00a9 DBD S 32%G&) FF Q S \u00a9 Wed Jul2 10:58AM\n\n= List of research\n\n\u20ac > C\n\n@ O\n\nGoogle Al Studio\n\n& Artificial Intellige CO NLP_1.ipynb - Co\n\nee eee eT oe\n\nCO NLP_1.ipynb - Co\n\nBB LAUNCHED Globe = M Launched - Artif\n\nArtificial_Intelligence\n\nO 6 =\n\naistue\u2014\n\nria | PyCharm\n\nQ Search projects\n\nNew |\n\n| to \\ EXCUSE-GENERATOR\nP > __pycache__\nVv env\n> bin\n> include\n> lib\n\u00a9 pyvenv.cfg U\n\n\u00a9\n\nEXPLORER\n\n .env\n@ excuse_api.py 5,U\n{} history.json\n\nme\nL&\nES\nA\n\n= requirements.txt U\n\n@\n$03 > OUTLINE\n\n> TIMELINE\n\n& maint @ @O0AS\nView status\n\n\u20ac >\n\nx Welcome @ excuse_api.py 5,U X % .env U\n\u00ae excuse_api.py > \u00a9 generate\n\n49 def generate(r: Req):\n\nPP excuse-generator\n\nee a\n\nDy B\n\n58 if response_text.startswith(\"***\"):\n59 response_text = response_text.strip(\"*\").lstrip(\"json\").strip|\n60 out = json. loads(response_text)\n61 entry = { 7\n62 \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\nPROBLEMS @ OUTPUT DEBUGCONSOLE TERMINAL PORTS tv A xX\nreturn retry_target( = LJ python3.11\nAAAARAAAAAAAA, 1 >] zsh\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\"\n, line 156, in retry_target '\nnext_sleep = _retry_error_helper( |\nFile \"/Users/bhuvanarora/excuse-generator/.venv/1lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", \u00a7\nline 214, in _retry_error_helper ny\nraise final_exc from source_exc\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\" =\n, line 147, in retry_target 1\nresult = target()\nAAAAAAAA I\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130\n\n, in func_with_timeout\nreturn func(*args, +kkwargs)\n\nFile \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", lin #\n\ne 78, in error_remapped_callable\nraise exceptions. from_grpc_error(exc) from exc\n\ngoogle.api_core.exceptions.NotFound: 404 models/gemini-pro is not found for API version vibeta, or is not supported\nfor generateContent. Call ListModels to see the list of available models and their supported methods.\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http://127.0.0.1:8000/generate \\\n\n-H \"Content-Type: application/json\" \\\n-d '{\"scenari missed class\",\"urgency\":\"panic\"\nInternal Server Errorg\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl\n\n-H \"Content-Type: application/json\" \\\n-d '{\"scenario\":\"missed class\",\"urgency\":\"panic\"\u2122\nInternal Server Errorg\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl\n\n-H \"Content-T:\n-d '{\"scenaris\nInternal Server Errorg\n\n: application/json\" \\\nmissed class\",\"urgency\":\n\npanic\"\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl\n\n-H \"Content-Type: application/json\" \\\n-d '{\"scenario\":\"missed class\",\"urgency\":\"panic\"\u2122\nInternal Server Errorg\n\n\u00ae (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl\n\n-H \"Content-Type: application/json\" \\\n-d '{\"scenario\":\"missed class\",\"urgency\":\"panic\"\u2122\ncurl:\n\n(28) Failed to connect to 127.0.0.1 port 8000 after 7781 ms:\n(.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % o\n\n-X POST http://127.0.@.1:8000/generate \\\n\n-X POST http://127.0.@.1:8000/generate \\\n\n-X POST http://127.0.@.1:8000/generate \\\n\n|\n|\n-X POST http://127.0.0.1:8000/generate \\ |\n|\nCouldn't connect to server\nLt\nQ  -n61,Col14 Spaces:4 UTF-8 LF {} Python &\n\na |\n\nLT omy Tt Tie eseon |\n\n3.13.2 64-bit O\n\n[| Al-05-BBLEN4 \u00a9 OpenAl coy API keys - Open/ (S) Gemini API refere +\u201d Get API key | GX\nIntelligent Excuse Generator Plan > \u2014\nBhuvan\n= out = json.loads(model.generate_content([full_promp+?. +ov+)\n\n52\n53\n54\n55\n56\n57\n58\n59\n\n60\n61\n62\n\nReview \u00a7 Revert\n\nmessages = [\nSystemMessage(content=SYSTEM_PROMPT) ,\nHumanMessage(content=full_prompt)\n\n]\n\nresponse_text = 1lm(messages).content.strip()\n\nif response_text.startswith(\"***\"):\n\nresponse_text =\nresponse_text.strip(\"\"\").1strip(\"json\").strip()\n\nout = json.loads(response_text)\nentry = {\n\"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n\nAll set! The FastAPI service now uses the LangChain Gemini wrapper (same approach as your\nactivity-analyzer script).\n\n1. Restart the server:\n\nbash O) Copy\nCtr1-C\nuvicorn excuse_api:app --reload\n2. Test:\nbash O) Copy\n\ncurl -X POST http://127.0.0.1:8000/generate \\\n\n-H \"Content-Type: application/json\" \\\n-d '{\"scenario\":\"missed class\", \"urgency\":\"panic\"}'\n\nYou should finally receive a JSON excuse. v\n\n\u00a9 excuse_api.py x\n\nMessage Bhuvan\n\n+ @ &"
}