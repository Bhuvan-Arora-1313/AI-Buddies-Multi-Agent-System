{
  "timestamp": "2025-07-01_17-49-39",
  "active_window": "firefox",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "git push",
  "vscode_text": "<!DOCTYPE html>\n<html>\n  <body style=\"margin: 0; background: rgba(0,0,0,0.7); color: white; font-family: sans-serif;\">\n    <div style=\"-webkit-app-region: drag; padding: 10px; background: rgba(0,0,0,0.8);\">\n      <pre id=\"output\" style=\"white-space: pre-wrap; word-wrap: break-word;\">Waiting for data...</pre>\n    </div>\n    <script>\n      const { ipcRenderer } = require('electron');\n      ipcRenderer.on('update-prediction', (event, data) => {\n        document.getElementById('output').textContent = data;\n      });\n    </script>\n  </body>\n</html>",
  "ocr_text": "= List of research papers - Google X\n\nC a\n\nY ChatGPT OX\n\nChatGPT v G\n\nSaved memory full \u00a9\n\n@ When Drones Stay Closer\n(Like a Flock):\ne Purpose: Cooperate tightly for a goal.\ne Examples:\ne Light shows\nCargo transport in groups\n\nDefense swarms attacking together\nIn these cases, moving together like\n\nbirds is useful.\n\n\u00bb Key Point:\n\n\u00ab Swarm behavior # Always flying close.\nIt means they coordinate and cooperate.\nSometimes they spread out, sometimes they\n\nfly together\u2014it depends on the mission.\n\nDo you want me to explain how drones stay\n\nconnected when the \\ \u2018e dispersed?\ni a rs RK\n\nAsk anything\n\n+ 2 Tools Q\n\nChatGPT can make mistakes. Check important info. See Cookie\n\nPreferences.\n\nN_ Enhanced multi agent coordinat X\n\n&\n\n* s41598-025-88145-7-2.pdf\n\naw\n\nVv\n\n9 of 17\n\nx\n\n\u00a9) Jaykap-Git/Buday\n\nOD ffile:///Users/bhuvanarora/Downloads/s41598-025-88145-7-2.pdf\n\nw\n\n\u2014 + 160% v\n\nobstacle avoidance strategies. It also uses the Bayesian optimization method to dygiiearenacsets\nsuch as learning rate and discount factor to improve the robustness of the algorith C) eee\n\nEnhanced multi-agent cluster model construction\n\nThickness\n\nBefore building the model, this study set the learning rates of the DQN and PPq s\n0.0003. The learning rate of DQN is mainly based on the convergence requirey\na dynamic environment, and a higher learning rate is selected to accelerate init \u00aba\nrate of PPO is based on empirical values and experimental tuning results, an\n\nselected to avoid oscillation or instability caused by too fast gradient updates in complex scenarios.\nThis study also set the batch size of DQN and PPO to 64 and 128. The smaller batch size of DQN is\nintended to improve the real-time update capability of the model, while the larger batch size of PPO helps\nto enhance the adaptability to complex environments and training stability. The discount factor is used\nto weigh short-term rewards and long-term benefits. This study set it to 0.99, highlighting the overall\nperformance of the drone swarm in long-term patrol missions and retaining the effectiveness of short-\nterm path optimization. The target network of the DQN algorithm is updated every 100 iterations. Based\non the consideration of model training stability, it delays the update of the target network to avoid training\noscillations caused by frequent updates. The PPO algorithm uses gradient clipping technology and sets the\ngradient clipping threshold to 0.2 to ensure that the gradient update does not exceed a reasonable range,\nthereby improving the stability of the model.\n\nShow all\n\nIn order to verify the effectiveness and correctness of the algorithm, this study also built a multi- drone motion\nsimulation model and uses the four-degree-of-freedom 8-state drone dynamics model\u2019. It contains 8 state\nvariables [s1, s2, s3, s4, s5, s6, s7, s8] and four input variables: the aileron deflection command Qa, elevator\nand rudder deflection command @y, rudder angle command 9, and throttle command Og. In order to control\nthe drone swarms, this study uses three cluster control quantities: speed V;, yaw angle R, and altitude H;. The\ndynamic model of the drone with attitude control capability is controlled through three cluster control quantities.\nThe output command of the cluster algorithm is converted into the input of the attitude control loop in the drone\ndynamic model. The PI control loop needs to be built as the transition. The relationship between the cluster\nalgorithm, PI control loop, attitude control loop and state dynamics model is shown in Fig. 6. In the process\nof model implementation, it first constructs a four-degree-of-freedom drone dynamics model. By defining\nthe drone\u2019s 8 state variables and 4 input variables, the drone dynamics equation is established to describe the\nrelationship between the state variables and the input variables. Then it designs the PI control loop by setting the\nproportional gain (K,) and integral gain parameters (K;) of the PI controller, which inputs the output command\nof the cluster algorithm, calculates the error (e) and the integral of the error (f edt), and calculates the control\noutput through the formula u (t) = Kpe(t) + Ki fe(t) dt. Finally, the attitude control loop is implemented.\nIt converts the output command of the PI controller into the attitude control command of the drone to control\nthe pitch angle, yaw angle and throttle, which achieves the precise control of the drone movement.\n\nExperimental result\n\nIn order to verify the performance of the EN-MASCA algorithm, it compares with the unimproved MASCA\n(multi-agent swarm control algorithm), NNCA (Nonlinear Neural Control Algorithm)\u201d, and NSGAII (Non-\ndominated Sorting Genetic Algorithm II)\u201d algorithm. The results are as follows:"
}