{
  "timestamp": "2025-07-02_12-53-13",
  "active_window": "Electron",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "curl -X POST http://127.0.0.1:8000/generate \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"scenario\":\"missed class\",\"urgency\":\"low\",\"voice\":true}'",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv(), override=True)\n\n\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\n# --- Gemini via LangChain (REST v1) ---\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.schema import SystemMessage, HumanMessage\n\n# expose key for LangChain wrapper\nos.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")  # must be in .env\n\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",          # free\u2011tier model\n    temperature=0.8,\n    convert_system_message_to_human=True\n)\n#print(\">>> model being used:\", model._name) \n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n    mode: str = \"normal\"   # \"normal\" | \"apology\"\n    language: str = \"en\"   # ISO code, e.g. \"en\", \"es\", \"fr\"\n    voice: bool = False   # If true, return an MP3 of the excuse\n\nclass EmergencyRequest(BaseModel):\n    number: str\n    message: str\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    style_clause = (\n        \"Respond in a guilt\u2011tripping, heartfelt apology tone.\"\n        if r.mode.lower() == \"apology\"\n        else \"Respond in a neutral, believable tone.\"\n    )\n    # language directive\n    lang_clause = (\n        \"\" if r.language.lower() in [\"en\", \"english\"] else\n        f\"Respond in {r.language} language.\"\n    )\n    full_prompt = (\n        f\"{SYSTEM_PROMPT}\\n\"\n        f\"Tone: {style_clause}\\n\"\n        f\"{lang_clause}\\n\"\n        f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    )\n    messages = [\n        SystemMessage(content=SYSTEM_PROMPT),\n        HumanMessage(content=full_prompt)\n    ]\n    response_text = llm(messages).content.strip()\n    # LangChain may wrap JSON in ``` blocks \u2013 strip them\n    if response_text.startswith(\"```\"):\n        response_text = response_text.strip(\"`\").lstrip(\"json\").strip()\n    out = json.loads(response_text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n    # --- optional voice synthesis ---\n    if r.voice:\n        from gtts import gTTS\n        audio_dir = Path(\"audio\")\n        audio_dir.mkdir(exist_ok=True)\n        audio_file = audio_dir / f\"{entry['id']}.mp3\"\n        gTTS(out[\"excuse\"], lang=r.language[:2]).save(audio_file.as_posix())\n        entry[\"audio\"] = str(audio_file)\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]\n\n# ---------- /emergency ----------\n@app.post(\"/emergency\")\ndef emergency(req: EmergencyRequest):\n    \"\"\"\n    Simulate sending an emergency SMS/call.\n    For the demo we just log the request and append an entry to history.json.\n    \"\"\"\n    entry = {\n        \"id\": f\"emergency-{int(time.time())}\",\n        \"ts\": time.time(),\n        \"excuse\": \"EMERGENCY TRIGGER\",\n        \"believability_score\": 1.0,\n        \"chat_log\": f\"Sent '{req.message}' to {req.number}\"\n    }\n    history = json.loads(DATA.read_text())\n    history.append(entry)\n    DATA.write_text(json.dumps(history, indent=2))\n    return {\"status\": \"sent\", \"to\": req.number, \"msg\": req.message}",
  "ocr_text": "@ Code File Edit Selection View Go Run Terminal Window\u2019 Help\n\nCore Features Designed:\n\n1. Al-Generated Excuses \u2014 Context-based excuse suggestions (work, school, social, family).\n\n2. Scenario-Based Customization \u2014 Allows users to refine excuses based on urgency and\n\ne@e@ < PP excuse-generator By oe a\n\noO EXPLORER 990 @ excuse_api.py 6 X  {} history.json M @ README.md .env U v\n\n\\ EXCUSE-GENERATOR \u00ae excuse_api.py > \u00a9 generate\n> __pycache__ dAf mananntalons Nanls Eu\nVv env PROBLEMS @ OUTPUT DEBUGCONSOLE TERMINAL PORTS tv A xX\n> Lelie //127.0.0.1:8000/generate \\\n(10K) > include -H \"Content-Type: application/json\" \\\n\n> lib -d '{\"scenario\":\"deadline\",\"urgency\":\"medium\",\"mode\":\"humorous\"}'\npyvenv.cfg\n\n-) python3.11\n>-} curl\n\nInternal Server Errorg\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http\nBey U //127.0.0.1:8000/generate \\\n\u00ae excuse_api.py 6 -H \"Content-Type: application/json\" \\\n\n-d '{\"scenario\":\"missed class\",\"urgency\":\"panic\",\"mode\":\"apology\"}'\n\n{} history.json M\n@ README.md {\"id\":\"af4f89590036448a16a258ad9637681\", \"ts\": 1751437937.811754,\"excuse\":\"0h\n. my god, I am so, so terribly sorry for missing class today. I truly feel awfu\n\n= requirements.txt 1. Something completely unexpected and deeply upsetting came up with my grand \u00a9 =\nmother \u2014 an urgent situation that required me to drop everything and rush to\nher side. I had absolutely no time to even send a message. I'm still so shake\nn.\",\"believability_score\":0.88,\"chat_log\":{\"sender_a\":\"Okay, hurry. She needs\nyou.\",\"me\":\"On my way! Just left, praying she's okay. So sorry I can't be th\nere sooner.\"}}2\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http\n//127.0.0.1:8000/generate \\\n-H \"Content-Type: application/json\" \\\n\n-d '{\"scenario\":\"perdi el autobis\",\"urgency\":\"medium\",\"language\":\"es\"}' \u00a9\nt\n{\"id\" :\"3c06e83e2933c79380a36489cb483bce\", \"ts\": 1751439526. 407582,\"excuse\":\"Mi \u00ab5\nperro se puso mal de repente y tuve que llevarlo al veterinario de urgencia. ,\nAcabamos de volver, pero perdi el autobis por eso.\",\"believability_score\":0.9\n,'\"chat_log\":\"Yo: Uf, el pobre Firulais se puso mal. Tuve que salir corriendo\nal vete.\\nAmigo: Joder, \u00e9esta bien?\\nYo: Si, ya lo revisaron, solo un susto,\npero me hizo perder el bus.\"}2\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http\n//127.0.0.1:8000/generate \\\n\n-H \"Content-Type: application/json\" \\\n\n-d '{\"scenario\":\"en retard\",\"urgency\":\"low\",\"mode\":\"humorous\",\" language\"\nifr}! q\n{\"id\" :\"'c2924ad30940ad24cada5823d3a9eaec\", \"ts\": 1751439855. 9286458, \"excuse\": \"D\u00e9\nsol\u00e9 pour le retard. Il y a eu un incident technique impr\u00e9vu sur ma ligne de\ntransport en commun habituelle, ce qui a engendr\u00e9 de gros retards et m\u2018a forc\n\u00e9 @ prendre un d\u00e9tour significatif.\",\"believability_score\":0.9,\"chat_log\":\"Am\nie: 'Pur\u00e9e, la ligne X est bloqu\u00e9e ce matin ! Quelle gal\u00e9re. '\\nMoi\ndessus, c'est L'enfer. Je vais \u00e9tre super en retard a cause de ca...'\"}2 :\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http\n//127.0.0.1:8000/generate \\\n-H \"Content-Type: application/json\" \\ t\n-d '{\"scenario\":\"missed flight\",\"urgency\":\"medium\",\"voice\":true}\nInternal Server Errorg\n(.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http: =\n//127.0.0.1:8000/generate \\\n> OUTLINE -H \"Content-Type: application/json\" \\\n\n-d '{\"scenario\":\"missed class\",\"urgency\":\" low\", \"voice\": true}\n> TIMELINE |\n\nLx % main* O @o0A6 \u00a9 Not Committed Yet QQ  Ln92,Col36 Spaces:4 UTF-8 LF {} Python @ 3.13.264-bit Q\n\nRSET TN Nearer NSIT ner snarenntner se rere NUTS TST STS neva ESS Svs NSE ESSN ST Sen 1B\n\n3 08:54 _\u2014\n\nPrevious 30 Days\n\nB= \u2014 1 = | EB\n\n98O\u00b0SGEE20\n\nG@ 8 OCOD S nxt F Q SB \u00a9 WedJul2 12:53PM\n\nMaccy\n\n\\@  -H \"Content-Type: application/json\" \\@\n\n\u00a5 @ curl -X POST http://127.0.0.1:8000/generate\n\nIntelligent Excuse Generator Plan >\nBhuvan\n\nQuick fix\n1. Install gTTS inside your virtual-env\n\nbash\n\nsource .venv/bin/activate\npip install gTTS\n\n2. Freeze requirements again\n\nbash\n\npip freeze > requirements.txt\n3. Restart Uvicorn\n\nbash\n\nCtr1-C\nuvicorn excuse_api:app --reload\n\n4. Test with voice flag\n\nbash\n\ncurl -X POST http://127.0.@.1:8000/generate \\\n-H \"Content-Type: application/json\" \\\n-d '{\"scenario\":\"missed class\", \"urgency\":\"low\",\"voice\":true}'\n\nYou should get a JSON response that now includes:\n\n\u00a9 excuse_api.py x\nMessage Bhuvan\n\n+ @\u00ae ov\n\n. . .\n\u2014\u2014\u2014\u2014 eee: eee \u2014\u2014\u2014\u2014\n\n-d '{\"scenario\":\"missed\nclass\",\"urgency\":\"low\",\"voice\":true}'\n\n\u00a9) Copy\n\n\u00a9) Copy\n\n\u00a9) Copy\n\nY Copied\n\n=]"
}