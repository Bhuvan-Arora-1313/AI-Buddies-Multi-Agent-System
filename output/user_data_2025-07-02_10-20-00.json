{
  "timestamp": "2025-07-02_10-20-00",
  "active_window": "Electron",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "+import google.generativeai as genai\n+genai.configure(api_key=os.getenv(\"GEMINI_KEY\"))\n+model = genai.GenerativeModel(\"gemini-pro\")",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv\nload_dotenv()          # \u2190 NEW: pulls OPENAI_API_KEY from .env\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nimport google.generativeai as genai\ngenai.configure(api_key=os.getenv(\"GEMINI_KEY\"))\nmodel = genai.GenerativeModel(\"gemini-pro\")\n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    prompt = f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    res = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\",   \"content\": prompt},\n        ],\n        temperature=0.8,\n    )\n    out = json.loads(res.choices[0].message.content)\n\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]",
  "ocr_text": "@ Code File Edit Selection View Go Run Terminal Window Help S6\u00a9 \u20ac8 \u00a9GD O S 2 F Q S \u00a9 Wed Jul2 10:20AM\n= List of research @ Artificial Intellige CO NLP_1.ipynb - Co CO NLP_1.ipynb - Co ca LAUNCHED Glob M Launched - Artif Artificial_Intelligence [| Al-05-BBLEN4 \u00a9 OpenAl coy API keys - Open/ (S) Gemini API refer +\u201d Get API key | GX\nO<\u00abvr cea O 8 & aistudio.google.com/apikey Intelligent Excuse Generator Plan > \u2014\nBhuvan\nc5oe3e & > PP excuse-generator By ee * .\nS 4 Edit excuse_api.py \u2122\no | EXPLORER J Welcome \u00ae excuse_api.py 5,U @ %%.env. U Dy\n= \\ EXCUSE-GENERATOR @ excuse_api.py > [e] model Repl the O Al block with G woe\nepiace the en. OCK WI emini.\n(\u00a9) {> \u2014Pycache_ 36 def generate(r: Req): | p P\nVv env ' all i ' i :\n\u00ae a 42 {\"role\": \"user\", \"content\": prompt}, | Here's the minimal diff (copy exactly what's between the lines):\n|\n> include 43 ] , 7 diff oO Copy\n| 2b 44 temperature=0.8,\n& \u00a9 pyvenv.cfg U\n .env U 45 ) ee .\nFS @ excuse_apipy 5,U 46 out = json. loads(res.choices[Q] .message. content) -from openai import OpenAl\nainictontisen 5 47 -client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\") )\n= requirements.txt U +import google.generativeai as genai\n48 entry = { +genai.configure(api_key=os.getenv(\"GEMINI_KEY\") )\n49 \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(), Ce RS ae ees ee re SO\n50 \"ts\": time.time(),\nand further down, swap the LLM call:\n51 **\u00abOUt,\n52 } diff \u00a9 Copy\n53\n54 history = json. loads(DATA. read_text() ) - res = client.chat.completions.create(\n55 if entry[\"id\"] not in {h[\"id\"] for h in history}: # de-dupe 2 Lie '\n. aa messages=\n56 history.append(entry) = {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n57 DATA.write_text(json.dumps(history, indent=2) ) - {\"role\": \"user\", \"content\": prompt},\n58 F 1,\n- temperature=0.8,\n59 return entry = )\n60 = out = json.loads(res.choices[0@].message.content)\n6 oneS + full_prompt = f\"{SYSTEM_PROMPT}\\n{prompt}\"\nLl # \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 /top?n=5 \u2014-----\u2014-\u2014\u2014\u2014 + out = json.loads(model.generate_content(full_prompt).text)\nWw) faann aat(\"/tan!\"!)\nPROBLEMS @) OUTPUT DEBUGCONSOLE TERMINAL PORTS +yv A x . . .\n(Everything else\u2014history file, FastAPI routes\u2014stays the same.)\nUsing cached httplib2-0.22.0-py3-none-any.whl (96 kB) aa) BJzsh\nUsing cached pyparsing-3.2.3-py3-none-any.whl (111 kB) >] zsh\nUsing cached uritemplate-4.2.0-py3\u2014-none-any.whl (11 kB) ~\nInstalling collected packages: urllib3, uritemplate, pyparsing, pyasn1, protobuf, grpcio, charset_normalizer, celica Save the file\nools, rsa, requests, pyasni-modules, proto-plus, httplib2, googleapis\u2014-common-protos, grpcio-status, google-auth, goon .\n@ gle-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\nSuccessfully installed cachetools-5.5.2 charset_normalizer-3.4.2 google-ai-generativelanguage-0.6.15 google-api-corey a\n-2.25.1 google-api-python-client-2.174.@ google-auth-2.4@.3 google-auth-httplib2-0.2.@ google-generativeai-0@.8.5 goo\n> OUTLINE gleapis\u2014common-protos-1.70.@ grpcio-1.73.1 grpcio-status-1.71.2 httplib2-0.22.@ proto-plus-1.26.1 protobuf-5.29.5 Py, v\n$03 asn1-@.6.1 pyasni-modules-0.4.2 pyparsing-3.2.3 requests-2. i 4 rsa-4.9.1 uritemplate-4.2.@ urllib3-2.5.0\n> TIMELINE (.venv) bhuvanarora@Bhuvans-MacBook-Pro excuse-generator % []\nwy % main* \u00ae@ @O0A5 Ln11,Col1 Spaces:4 UTF-8 LF {} Python \u00ae 3.13.264-bit \u00a9\n\u2014 = _ Message Bhuvan\n\u00ab +O \u00a2 @\n3\n\nfile.pdf\n\nITRV (1).pdf ITRV.pdf AIEUTIR EAI EY Iko to mtj Iko\n\nS80\u00b0G8880 eee 7a\n\nto MTJ (1).pdf\n\nIko to MTJ-\\-\n\nae"
}