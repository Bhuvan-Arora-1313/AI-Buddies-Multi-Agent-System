{
  "timestamp": "2025-07-02_08-26-23",
  "active_window": "firefox",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "Smart Border Patrol Using Drones and Wireless Charging System Under Budget Limitation\n",
  "vscode_text": "<!DOCTYPE html>\n<html>\n  <body style=\"margin: 0; background: rgba(0,0,0,0.7); color: white; font-family: sans-serif;\">\n    <div style=\"-webkit-app-region: drag; padding: 10px; background: rgba(0,0,0,0.8);\">\n      <pre id=\"output\" style=\"white-space: pre-wrap; word-wrap: break-word;\">Waiting for data...</pre>\n    </div>\n    <script>\n      const { ipcRenderer } = require('electron');\n      ipcRenderer.on('update-prediction', (event, data) => {\n        document.getElementById('output').textContent = data;\n      });\n    </script>\n  </body>\n</html>",
  "ocr_text": "Firefox File Edit View History Bookmarks Tools Window Help \u00a9 \u20ac&8 \u00a9 BD O 8S axe\n\n(ica) =} List of research papers - Google X A UAV patrol system using pano X * A UAV patrol system using par X Smart border patrol using drone X +\n\nC a 0 8 sciencedirectassets.com\n\nA Vv \u2014 + 200%\n\nIme\n\nHighlight colour\n\n: Tmax \u2014 Tmin a\nVv AUAV patrol system using @ Highlight removed x \u00a9 @ e @\n\npanoramic stitching and\n\nobject detection\n\n1 Introduction Thickness\nVv 2 Related work e\n\n2.1 Image stitching\n\n2.2 Motion ghost 3.4.3. Model structure\nelimination In the network structure of the Faster R-CNN approach, the object classification layer receives regions\u00ae\u2122\u00ae\n2.3 Object detection . : . . .\n\n2 Su pstalereen and feature maps, to produce the final results which include object bounding boxes, the object category and confidence\n3.1 System scores. In the initial part of this layer, ROIs from the proposed target layer and 1024 feature maps from the head network\narchitecture pass through pooling layers and a fully connected layer to generate the output as illustrated in\n3.2 Image stitching : : : : : . . . .\nag Misti ceecn There are two paths in this network. The first is for scoring objects with a classifier, in which cls_score_net produces\nelimination scores for each predicted bounding box regarding each object class. These scores then pass through softmax and max func-\n\n. \u201cee detection tions to obtain probability values and to determine the most likely class. Meanwhile, bbox_pred_net and bbox_transform_inv\n4. cene\n\nShow all\n\nparsing on the second path use regression coefficients to obtain more accurate predictions of bounding box locations for each object\n\n3.4.2 Quantifying class.\nthe object-scene\n\nrelationship For the object classification score adjustment, the scene category of every predicted bounding box is determined by the\n8.4.3 Model central pixel\u2019s scene parsing result. Compared to calculating the mode number of pixels in the bounding box, using the\n\nstructure\n\n4. Experiments center pixel only may be slightly less representative but shows higher performance (over 10 times FPS). The corresponding\nY 4.1 Dataset and confidence value V;; is then applied to the score with a certain weight factor f.\n\nexperiment settings\n4.1.1 Dataset\n\n4.1.2 Experiments\nsettings\n\n4.2 Image stitching\nevaluation\n\n4.3 Object detection\nevaluation\n\n4.4 Results\n5 Conclusion\n\nDeclaration of\nCompeting Interest\n\nReferences Y. Zhou, T. Rui and Y. Li et al./Computers and Electrical Engineering 80 (2019) 106473\n\nSS8O00eEGSE20 O82 E*"
}