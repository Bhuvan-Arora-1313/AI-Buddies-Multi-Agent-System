{
  "timestamp": "2025-07-02_12-01-14",
  "active_window": "Electron",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "/Users/bhuvanarora/excuse-generator/excuse_api.py:73: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  response_text = llm(messages).content.strip()\n/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\nINFO:     127.0.0.1:65462 - \"POST /generate HTTP/1.1\" 500 Internal Server Error\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 112, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 214, in run_endpoint_function\n    return await run_in_threadpool(dependant.call, **values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/starlette/concurrency.py\", line 37, in run_in_threadpool\n    return await anyio.to_thread.run_sync(func)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/excuse-generator/excuse_api.py\", line 77, in generate\n    out = json.loads(response_text)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/.pyenv/versions/3.11.13/lib/python3.11/json/__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/.pyenv/versions/3.11.13/lib/python3.11/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/bhuvanarora/.pyenv/versions/3.11.13/lib/python3.11/json/decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
  "vscode_text": "# excuse_api.py\nimport os, json, time, hashlib\nfrom pathlib import Path\nfrom dotenv import load_dotenv, find_dotenv\nload_dotenv(find_dotenv(), override=True)\n\n\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\n# --- Gemini via LangChain (REST v1) ---\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.schema import SystemMessage, HumanMessage\n\n# expose key for LangChain wrapper\nos.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")  # must be in .env\n\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",          # free\u2011tier model\n    temperature=0.8,\n    convert_system_message_to_human=True\n)\n#print(\">>> model being used:\", model._name) \n# ---------- simple \u201cDB\u201d ----------\nDATA = Path(\"history.json\")\nif not DATA.exists():\n    DATA.write_text(\"[]\")          # seed empty list\n\n\n\n# ---------- FastAPI app ----------\napp = FastAPI()\n\nclass Req(BaseModel):\n    scenario: str  # e.g. \"missed class\"\n    urgency: str   # \"low\" | \"medium\" | \"panic\"\n    mode: str = \"normal\"   # \"normal\" | \"apology\"\n\nclass EmergencyRequest(BaseModel):\n    number: str\n    message: str\n\nSYSTEM_PROMPT = \"\"\"\nYou are an elite alibi-creator.\nReturn a JSON with:\n  excuse               (\u2264 50 words),\n  believability_score  (0-1),\n  chat_log             (short WhatsApp-style proof)\n\"\"\"\n\n# ---------- /generate ----------\n@app.post(\"/generate\")\ndef generate(r: Req):\n    # Tone selection based on mode\n    tone_map = {\n        \"normal\": \"Respond in a neutral, believable tone.\",\n        \"apology\": \"Respond in a guilt-tripping, heartfelt apology tone.\",\n        \"humorous\": \"Respond in a lighthearted, witty style.\",\n        \"professional\": \"Use formal business language and keep it concise.\",\n        \"sarcastic\": \"Answer with brief, dry sarcasm.\",\n        \"emergency\": \"Sound panicked and extremely urgent; maximum 20 words.\"\n    }\n    style_clause = tone_map.get(r.mode.lower(), tone_map[\"normal\"])\n    full_prompt = (\n        f\"{SYSTEM_PROMPT}\\nTone: {style_clause}\\n\"\n        f\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n    )\n    messages = [\n        SystemMessage(content=SYSTEM_PROMPT),\n        HumanMessage(content=full_prompt)\n    ]\n    response_text = llm(messages).content.strip()\n    # LangChain may wrap JSON in ``` blocks \u2013 strip them\n    if response_text.startswith(\"```\"):\n        response_text = response_text.strip(\"`\").lstrip(\"json\").strip()\n    out = json.loads(response_text)\n    entry = {\n        \"id\": hashlib.md5(out[\"excuse\"].encode()).hexdigest(),\n        \"ts\": time.time(),\n        **out,\n    }\n\n    history = json.loads(DATA.read_text())\n    if entry[\"id\"] not in {h[\"id\"] for h in history}:   # de-dupe\n        history.append(entry)\n        DATA.write_text(json.dumps(history, indent=2))\n\n    return entry\n\n# ---------- /top?n=5 ----------\n@app.get(\"/top\")\ndef top(n: int = 5):\n    history = json.loads(DATA.read_text())\n    history.sort(key=lambda x: x[\"believability_score\"], reverse=True)\n    return history[:n]\n\n# ---------- /emergency ----------\n@app.post(\"/emergency\")\ndef emergency(req: EmergencyRequest):\n    \"\"\"\n    Simulate sending an emergency SMS/call.\n    For the demo we just log the request and append an entry to history.json.\n    \"\"\"\n    entry = {\n        \"id\": f\"emergency-{int(time.time())}\",\n        \"ts\": time.time(),\n        \"excuse\": \"EMERGENCY TRIGGER\",\n        \"believability_score\": 1.0,\n        \"chat_log\": f\"Sent '{req.message}' to {req.number}\"\n    }\n    history = json.loads(DATA.read_text())\n    history.append(entry)\n    DATA.write_text(json.dumps(history, indent=2))\n    return {\"status\": \"sent\", \"to\": req.number, \"msg\": req.message}",
  "ocr_text": "@ Code File Edit Selection View Go Run Terminal Window\u2019 Help 6 8 \u00a9OD S xm F Q S \u00a9 Wed Jul2 12:01PM\n\n[|\nK 4\n\u00a5\nS)\ncc)\n\n\u00a9 9 Get API key |G X\n\nw usr os\n\nIntelligent Excuse Generator Plan >\n\nBhuvan\neee@ \u20ac PP excuse-generator By Ean Ww \u00a9) Copy\n| o EXPLORER vr \u00ae excuse_api.py5 X  {} history.json @ README.md env sU Dy\n\\ EXCUSE-GENERATOR \u00ae excuse_api.py > \u00a9 generate ting\",\n{2 > \u2014Pycache_ def generate(r: Req): |\nVv wvenv 7 ~ - - - 7 Ls |\n\nry > bin \"professional\": \"Use formal business language and keep it concise.\", FE:\n\neee \"sarcastic\": \"Answer with brief, dry sarcasm.\", =\n\n> ii z:\nFs: prrenvera \"emergency\": \u201cSound panicked and extremely urgent; maximum 2@ words.\"\n_ env U }\nCh) \u00ae excuse_api.py 5\n\nstyle_clause = tone_map.get(r.mode. lower(), tone_map[\"normal\"] ) \u2018as many as you like by extending that dictionary.\nfull_prompt = (\n\nf\"{SYSTEM_PROMPT}\\nTone: {style_clause}\\n\"\n\nf\"Scenario: {r.scenario}\\nUrgency: {r.urgency}\"\n\n{} history.json\nJX @ README.md 65\n\n= requirements.txt\n\n) add the suggested modes directly in vs\n\nPROBLEMS @ OUTPUT DEBUG CONSOLE TERMINAL PORTS +v we NOX\n\nd I had to rush them to urgent care. It was incredibly sudden and chaotic, pure panic. I'm still at the clinic with them, waiting for u,\n\npdates. Couldn't even think about class.\",\"believability_score\":0.96,\"chat_log\":{\"Mom\":\"Hurry! They're struggling to breathe.\",\"You\":\"Pa\nulling up now. This is insane.\"}}2 1\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http://127.0.@.1:8000/emergency \\\n\n-H \"Content-Type: application/json\" \\ F;\n\n-d '{\"number\" 1123456789\" ,,\"\"message\":\"Call me now!\"}' 1\n{\"status\":\"sent\",\"to\":\"+1123456789\",\"msg\":\"Call me now!\" }2 1 fa)\n\n-) python3.11\n\n>-} curl\n\nReview  \u00a7& Revert\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http://127.0.@.1:8000/generate \\\n-H \"Content-Type: application/json\" \\ I\n\nd '{\"scenario\":\"missed class\",\"urgency\":\"panic\",\"mode\":\"apology\"}' .\n\n{\"id\" :\"b3d467d71332169893e42b24102ece48\", \"ts\":1751437206.329449,\"excuse\":\"Woke up in absolute agony with what felt like severe food poi\n\nsoning. Spent the entire morning completely incapacitated and glued to the bathroom. There was simply no way I could have left the hous\n\ne without risking a truly embarrassing public incident.\",\"believability_score\":0.92,\"chat_log\":\"Friend: Hey, you good? Missed you in cl\n\nass.\\nMe: OMG, so sorry! Woke up with the worst stomach bug. It was pure hell, literally couldn't move from the bathroom. Still feeling\n\nAes \" ' 4 i i h . .\n\ncals Nae Ugh, that sucks! Hope you feel better soon. Don't worry about notes.\\nMe: Thanks. Seriously, thought I was dying. Appr ilt-tripping, heartfelt apology tone.\"\n\n\u00a9@ (.venv) bhuvanarora@Bhuvans-MacBook-Pro excuse-generator % curl -X POST http://127.0.0.1:8000/generate \\ \\ oa 1 1\n-H \"Content-Type: application/json\" \\ eS apology\n-d '{\"scenario\":\"bumped my friends car that i borrowed\",\"urgency\":\"low\",\"mode\":\"apology\"}' .\n\n{\"id\"':\"2d2e2aca2d3e7e9 fb66d8e4d5694613f\", \"ts\": 1751437389.9344718,\"excuse\":\"Hey, so sorry, but someone must have dinged your car while i a neutral, believable tone.\"\nt was parked at the grocery store. I left it for maybe 3@ minutes, came back, and saw the bump on the rear fender. I'm really gutted ab\u00ae\u2122\nout it, I swear I didn't feel anything while driving.\",\"believability_score\":@.88,\"chat_log\":\"Me: Hey man, got some annoying news about\n\nyour car...\\nFriend: Oh no, what?!\\nMe: Looks like someone backed into it or something while it was parked at Tesco. Found a bump on ty\nhe rear when I came out. I'm so sorry, dude.\\nFriend: Ugh, seriously?! People are the worst. Is it bad?\\nMe: Not terrible, but definite\nly noticeable. I'm gutted for you. Couldn't believe it.\"}2 '\n\n@ (.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http://127.@..1:8000/generate \\ i.\n-H \"Content-Type: application/json\" \\ a\n\n(2) -d '{\"scenario\":\"deadline\",\"urgency\":\"medium\",\"mode\":\"humorous\"}' t\nInternal Server Errorg\n\nin a neutral belie dle tone.\"\n\n(.venv) bhuvanarora@Bhuvans\u2014-MacBook-Pro excuse-generator % curl -X POST http://127.0.0.1:8000/generate \\ }\nsos > OUTLINE -H \"Content-Type: application/json\" \\\n\n-d '{\"scenario\":\"deadline\",\"urgency\":\"medium\",\"mode\":\"humorous\"}' i\n> TIMELINE |\n\nLx  mainx O @0A5 \u00a9 Not Committed Yet QQ Ln65,Col20 Spaces:4 UTF-8 LF {} Python @ 3.13.264-bit Q\n\n= \u2014 a + @\u00ae & \u00a2 @\n\nPrevious 30 Days\n\nB= \u2014 1 = | EB"
}