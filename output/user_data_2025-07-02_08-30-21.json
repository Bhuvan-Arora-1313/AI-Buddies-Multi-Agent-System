{
  "timestamp": "2025-07-02_08-30-21",
  "active_window": "firefox",
  "focused_text": "Could not extract AXValue from focused element",
  "clipboard": "Smart Border Patrol Using Drones and Wireless Charging System Under Budget Limitation\n",
  "vscode_text": "<!DOCTYPE html>\n<html>\n  <body style=\"margin: 0; background: rgba(0,0,0,0.7); color: white; font-family: sans-serif;\">\n    <div style=\"-webkit-app-region: drag; padding: 10px; background: rgba(0,0,0,0.8);\">\n      <pre id=\"output\" style=\"white-space: pre-wrap; word-wrap: break-word;\">Waiting for data...</pre>\n    </div>\n    <script>\n      const { ipcRenderer } = require('electron');\n      ipcRenderer.on('update-prediction', (event, data) => {\n        document.getElementById('output').textContent = data;\n      });\n    </script>\n  </body>\n</html>",
  "ocr_text": "@ Firefox Edit View History Bookmarks Tools Window Help \u00a9 8 \u00a9oB0Os 4e\n\nee (ica) = List of research papers - Google X A UAV patrol system using pano X * A UAV patrol system using par X Smart border patrol using drone X +\n\nCo 08 sciencedirectassets.com\nem) A Vv \u2014 + 200%\nd a CSUIT. y VIUVECU ad 5 UUUIC, VW Cc UCLE U UUUIT a U BD cd\nFaster R-CNN. In a real-world application, this system can function as a web server that accepts an uppcaohiai\nv A UAV patrol system using . . . .\npanoramic stitching and returns a stitched image with detection labels afterwards. C) eee\n\nobject detection\n\noo : ime\noo -\n\n1 Introduction Thickness\n\nVv 2 Related work 3.2. Image stitching e\n2.1 Image stitching\n\ncern ees We adopt SPHP as the image stitching algorithm, which works at a simple principle while providingsiaEi\n\nelimination\n\n2.3 Object detection effect at a satisfactory speed. SPHP proposes a method that can smoothly transition from a projection transformation fo\n\u00a5 3 UAV patrol system an overlapping region and to a similar transformation in a non-overlapping region, thus creating naturally looking stitching\nShi Syste results. However, this works best for the stitching of static objects. For moving objects in the image sequences or video,\n\narchitecture\n\n3.2 Image stitching motion residuals will appear in the stitched results, thus resulting in a bad visual experience.\n\n3.3 Motion ghost\nelimination\n\n\u00a5 3.4 Object detection 3.3. Motion ghost elimination\n\n3.4.1 Scene\nparsing\n\n3.4.2 Quantifying\n\nThe region growing method based on a difference image is an|effective method for the removal of ghosts of moving\nthe object-scene objects in stitched images. The difference image is obtained as the absolute value of pixel differences of two images after\n\nrelationship\n\n3.4.3 Model the image registration. For static objects, the pixel value of the difference image is close to (0, 0, 0), thus black when\nstructure visualized. Based on this information, we use the region growing method to find connected regions of non-black pixels, and\n\na ; can thus distinguish between moving regions and static regions.\nv 4.1 Dataset an\n\nexperiment settings For moving objects at the edge of the overlapping area, we select objects in the image that are close to the edge to cover\n4.1.1 Dataset the moving object and to prevent the moving object from being disconnected. For objects in the middle of the overlapping\n4.1.2 Experiments area, we select objects in the later image to cover the moving objects, as they reflect the latest position of the object. This\n\nsettings\n\n4.2 Image stitching method is not suitable for scenes with large movements and thus large distance, as this may cause duplicates of the moving\n\nluat : . . .\nSeon, ; object. The removal effect is shown in the Fig. 2.\n4.3 Object detection\n\nevaluation\n4.4 Results 3.4. Object detection\n\n5 Conclusion\n\nDeclaration of\n\nCompetinglinterest 3.4.1. Scene parsing\n\nReferences We adopt the Pyramid Scene Parsing Network |24] approach for pixel-level scene parsing with the model trained on\nthe ADE20K dataset, which includes 150 semantic categories. To increase the classification accuracy, scene type should be\nas relevant with the possibility of different type of objects\u2019 occurrences as possible. Therefore, these original categories\n\n*ERBE2O OO E\u00ae"
}